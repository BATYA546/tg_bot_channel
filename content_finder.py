# content_finder.py
import requests
import logging
from datetime import datetime
import time
from bs4 import BeautifulSoup
import random
import re

logger = logging.getLogger(__name__)

class ContentFinder:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        self.keywords = [
            "–ø–µ—Ä–≤", "—Ä–µ–≤–æ–ª—é—Ü", "—Ä–µ–∫–æ—Ä–¥", "–ø—Ä–æ—Ä—ã–≤", "–∏–Ω–Ω–æ–≤–∞—Ü", "–æ—Ç–∫—Ä—ã—Ç–∏–µ",
            "–∏–∑–æ–±—Ä–µ—Ç", "—Ç–µ—Ö–Ω–æ–ª–æ–≥", "–Ω–∞—É–∫", "–∏—Å—Ç–æ—Ä–∏", "—É–Ω–∏–∫–∞–ª—å–Ω", "–Ω–æ–≤—ã–π",
            "–ø—Ä–æ–≥—Ä–µ—Å—Å", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫", "—Å–æ–∑–¥–∞–Ω", "–∑–∞–ø—É—Å–∫", "–æ–±–Ω–∞—Ä—É–∂–µ–Ω"
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.sources = [
            {
                'name': '–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏ (–Ω–∞—É–∫–∞)',
                'url': 'https://ria.ru/science/',
                'parser': self.parse_ria_science
            },
            {
                'name': '–¢–ê–°–° (–Ω–∞—É–∫–∞)',
                'url': 'https://tass.ru/nauka',
                'parser': self.parse_tass_science
            },
            {
                'name': 'N+1 (–∞—Ä—Ö–∏–≤)',
                'url': 'https://nplus1.ru/news/archive',
                'parser': self.parse_nplus1_archive
            }
        ]

    def search_content(self, max_posts=5):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        found_content = []
        
        for source in self.sources:
            try:
                logger.info(f"üì∞ –ü—Ä–æ–≤–µ—Ä—è—é {source['name']}...")
                content = source['parser'](source['url'])
                if content:
                    found_content.extend(content)
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(content)} –∑–∞–ø–∏—Å–µ–π")
                
                if len(found_content) >= max_posts:
                    found_content = found_content[:max_posts]
                    break
                    
                time.sleep(3)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source['name']}: {e}")
                continue
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback - —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        if not found_content:
            found_content = self.generate_fallback_content()
        
        logger.info(f"üéØ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {len(found_content)}")
        return found_content

def parse_ria_science(self, url):
    """–ü–∞—Ä—Å–µ—Ä –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏ (–Ω–∞—É–∫–∞)"""
    try:
        response = self.session.get(url, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = []
        news_items = soup.find_all('div', class_=['list-item', 'cell'])
        
        for item in news_items[:15]:
            try:
                title_elem = item.find(['a', 'h2', 'span'])
                if not title_elem:
                    continue
                    
                title = title_elem.get_text().strip()
                if len(title) < 10:
                    continue
                
                if self.is_relevant_content(title):
                    article_data = {
                        'title': title,
                        'summary': self.generate_summary(title),
                        'category': self.categorize_content(title),
                        'url': '',  # –£–±—Ä–∞–ª–∏ source, –æ—Å—Ç–∞–≤–∏–ª–∏ url –ø—É—Å—Ç—ã–º
                        'found_date': datetime.now()
                    }
                    articles.append(article_data)
                    
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏ –†–ò–ê: {e}")
                continue
                
        return articles[:3]
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –†–ò–ê: {e}")
        return []

def parse_tass_science(self, url):
    """–ü–∞—Ä—Å–µ—Ä –¢–ê–°–° (–Ω–∞—É–∫–∞)"""
    try:
        response = self.session.get(url, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = []
        news_items = soup.find_all(['article', 'div'], class_=True)
        
        for item in news_items[:20]:
            try:
                title_elem = (item.find('h2') or 
                             item.find('h3') or 
                             item.find('a') or 
                             item.find('span'))
                
                if not title_elem:
                    continue
                    
                title = title_elem.get_text().strip()
                if len(title) < 15:
                    continue
                
                if self.is_relevant_content(title):
                    article_data = {
                        'title': title,
                        'summary': self.generate_summary(title),
                        'category': self.categorize_content(title),
                        'url': '',
                        'found_date': datetime.now()
                    }
                    articles.append(article_data)
                    
            except Exception as e:
                continue
                
        return articles[:2]
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¢–ê–°–°: {e}")
        return []

def parse_nplus1_archive(self, url):
    """–ü–∞—Ä—Å–µ—Ä –∞—Ä—Ö–∏–≤–∞ N+1"""
    try:
        response = self.session.get(url, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = []
        news_items = soup.find_all('article')[:10]
        
        for item in news_items:
            try:
                title_elem = item.find('h2') or item.find('a')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text().strip()
                
                if self.is_relevant_content(title):
                    article_data = {
                        'title': title,
                        'summary': self.generate_summary(title),
                        'category': self.categorize_content(title),
                        'url': '',
                        'found_date': datetime.now()
                    }
                    articles.append(article_data)
                    
            except Exception:
                continue
                
        return articles[:2]
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ N+1: {e}")
        return []

def generate_fallback_content(self):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"""
    logger.info("üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç...")
    
    fallback_titles = [
        "–£—á–µ–Ω—ã–µ —Å–æ–∑–¥–∞–ª–∏ –ø–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ –∫–≤–∞–Ω—Ç–æ–≤—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä —Å —Ä–µ–∫–æ—Ä–¥–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é",
        "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –ø–æ–∑–≤–æ–ª–∏–ª–∞ –≤–ø–µ—Ä–≤—ã–µ –ø–æ–ª—É—á–∏—Ç—å —ç–Ω–µ—Ä–≥–∏—é –∏–∑ –≤–∞–∫—É—É–º–∞",
        "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —á–∞—Å—Ç–∏—Ü–∞, –º–µ–Ω—è—é—â–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ —Ñ–∏–∑–∏–∫–µ",
        "–ü–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –ø—Ä–æ—à–µ–ª —Ç–µ—Å—Ç –¢—å—é—Ä–∏–Ω–≥–∞ —Å —Ä–µ–∫–æ—Ä–¥–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º",
        "–ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞: —Å–æ–∑–¥–∞–Ω—ã —Å–æ–ª–Ω–µ—á–Ω—ã–µ –±–∞—Ç–∞—Ä–µ–∏ —Å –ö–ü–î –±–æ–ª–µ–µ 50%"
    ]
    
    content_list = []
    for title in random.sample(fallback_titles, 3):
        content_list.append({
            'title': title,
            'summary': self.generate_summary(title),
            'category': self.categorize_content(title),
            'url': '',
            'found_date': datetime.now()
        })
    
    return content_list

# –í –º–µ—Ç–æ–¥–µ format_for_preview –º–µ–Ω—è–µ–º:
def format_for_preview(self, content):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
    # –°–æ–∑–¥–∞–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    category_emojis = {
        'technology': 'üîß',
        'science': 'üî¨', 
        'records': 'üèÜ',
        'discovery': 'üí°'
    }
    
    emoji = category_emojis.get(content['category'], 'üì∞')
    
    return f"""
{emoji} *–ù–æ–≤—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏*

*–ó–∞–≥–æ–ª–æ–≤–æ–∫:* {content['title']}
*–ö–∞—Ç–µ–≥–æ—Ä–∏—è:* {content['category']}

*–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:*
{content['summary']}

‚è∞ –ù–∞–π–¥–µ–Ω–æ: {content['found_date'].strftime('%H:%M %d.%m.%Y')}
        """

def setup_content_finder():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    return ContentFinder()
