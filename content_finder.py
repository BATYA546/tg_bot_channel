# content_finder.py
import logging
import requests
from datetime import datetime
import random
import hashlib
from bs4 import BeautifulSoup
import re
import json
import time
import urllib.parse
import feedparser

logger = logging.getLogger(__name__)

class ContentFinder:
    def __init__(self, db_manager=None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        self.db_manager = db_manager
        self.post_hashes = set()
        self.load_existing_hashes()
        
        # –†–µ–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.sources = [
            self.parse_google_news,
            self.parse_science_news,
            self.parse_tech_news,
            self.parse_historical_facts,
            self.parse_wikipedia_firsts
        ]

    def load_existing_hashes(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ö–µ—à–∏ –∏–∑ –ë–î –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if self.db_manager:
            try:
                conn = self.db_manager.get_connection()
                cursor = conn.cursor()
                cursor.execute('SELECT title, content FROM found_content')
                existing_posts = cursor.fetchall()
                
                for title, content in existing_posts:
                    text = title + content[:200]
                    content_hash = hashlib.md5(text.encode()).hexdigest()
                    self.post_hashes.add(content_hash)
                    
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.post_hashes)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ö–µ—à–µ–π –∏–∑ –ë–î")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ö–µ—à–µ–π –∏–∑ –ë–î: {e}")

    def search_content(self, max_posts=3):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        found_content = []
        attempts = 0
        max_attempts = 8
        
        while len(found_content) < max_posts and attempts < max_attempts:
            attempts += 1
            logger.info(f"üîç –ü–æ–ø—ã—Ç–∫–∞ {attempts}/{max_attempts}")
            
            random.shuffle(self.sources)
            
            for source in self.sources:
                try:
                    if len(found_content) >= max_posts:
                        break
                        
                    logger.info(f"üì° –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫: {source.__name__}")
                    content_list = source()
                    
                    if content_list:
                        for content in content_list:
                            if self.is_truly_unique_content(content) and len(found_content) < max_posts:
                                # –£–ª—É—á—à–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                                if not content.get('image_url') or 'unsplash' in content.get('image_url', ''):
                                    content['image_url'] = self.get_relevant_image(content['title'], content['summary'])
                                
                                found_content.append(content)
                                content_hash = self.get_content_hash(content)
                                self.post_hashes.add(content_hash)
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç: {content['title'][:50]}...")
                            
                            if len(found_content) >= max_posts:
                                break
                
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source.__name__}: {e}")
                    continue
                
            time.sleep(2)
        
        logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {len(found_content)}")
        return found_content

    def is_truly_unique_content(self, content):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ —Ö–µ—à–∏ –∏ –ë–î"""
        content_hash = self.get_content_hash(content)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ø–∞–º—è—Ç–∏
        if content_hash in self.post_hashes:
            logger.info(f"üö´ –ü–æ—Å—Ç —É–∂–µ –≤ –ø–∞–º—è—Ç–∏: {content['title'][:30]}...")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ë–î
        if self.db_manager and self.is_content_in_db(content):
            logger.info(f"üö´ –ü–æ—Å—Ç —É–∂–µ –≤ –ë–î: {content['title'][:30]}...")
            return False
            
        return True

    def is_content_in_db(self, content):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = self.db_manager.get_connection()
            cursor = conn.cursor()
            
            search_term = content['title'][:40]
            cursor.execute('''
                SELECT id FROM found_content 
                WHERE title LIKE %s OR content LIKE %s
            ''', (f"%{search_term}%", f"%{search_term}%"))
            
            result = cursor.fetchone()
            return result is not None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ë–î: {e}")
            return False

    def get_content_hash(self, content):
        """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        text = content['title'] + content['summary'][:150]
        return hashlib.md5(text.encode()).hexdigest()

    def parse_google_news(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ Google News –ø–æ —Ç–µ–º–∞—Ç–∏–∫–µ –ø–µ—Ä–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π"""
        try:
            articles = []
            
            keywords = [
                "–ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ", "–ø–µ—Ä–≤–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ", "—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ",
                "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ", "–ø—Ä–æ—Ä—ã–≤ –≤ –Ω–∞—É–∫–µ", "–ø–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ",
                "–≤–ø–µ—Ä–≤—ã–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏", "–Ω–æ–≤–∞—è —ç—Ä–∞", "–∑–Ω–∞–∫–æ–≤–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ"
            ]
            
            for keyword in random.sample(keywords, 3):
                try:
                    url = f"https://news.google.com/rss/search?q={urllib.parse.quote(keyword)}&hl=ru&gl=RU&ceid=RU:ru"
                    
                    response = self.session.get(url, timeout=15)
                    if response.status_code == 200:
                        feed = feedparser.parse(response.content)
                        
                        for entry in feed.entries[:5]:
                            title = entry.title
                            summary = entry.get('summary', '') or entry.get('description', '')
                            link = entry.link
                            
                            if self.is_relevant_content(title + summary):
                                full_content = self.get_article_content(link)
                                image_url = self.extract_image_from_article(link) or self.get_relevant_image(title, summary)
                                
                                formatted_post = self.format_news_post(title, full_content or summary, keyword)
                                
                                articles.append({
                                    'title': title,
                                    'summary': formatted_post,
                                    'category': 'news',
                                    'url': link,
                                    'image_url': image_url,
                                    'found_date': datetime.now()
                                })
                                
                                if len(articles) >= 3:
                                    return articles
                    
                    time.sleep(2)
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ {keyword}: {e}")
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Google News: {e}")
            return []

    def parse_science_news(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            articles = []
            
            science_feeds = [
                "https://naked-science.ru/rss.xml",
                "https://scientificrussia.ru/rss",
                "https://elementy.ru/rss",
                "https://www.popmech.ru/rss/"
            ]
            
            for feed_url in random.sample(science_feeds, 2):
                try:
                    response = self.session.get(feed_url, timeout=15)
                    if response.status_code == 200:
                        feed = feedparser.parse(response.content)
                        
                        for entry in feed.entries[:5]:
                            title = entry.title
                            summary = entry.get('summary', '') or entry.get('description', '')
                            link = entry.link
                            
                            if self.is_relevant_content(title + summary):
                                image_url = self.extract_image_from_article(link) or self.get_relevant_image(title, summary)
                                
                                formatted_post = self.format_science_post(title, summary)
                                
                                articles.append({
                                    'title': title,
                                    'summary': formatted_post,
                                    'category': 'science',
                                    'url': link,
                                    'image_url': image_url,
                                    'found_date': datetime.now()
                                })
                                
                                if len(articles) >= 3:
                                    return articles
                    
                    time.sleep(2)
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–µ–Ω—Ç—ã {feed_url}: {e}")
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    def parse_tech_news(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            articles = []
            
            tech_feeds = [
                "https://3dnews.ru/news/rss/",
                "https://hi-news.ru/feed",
                "https://www.ixbt.com/export/news.rss",
                "https://habr.com/ru/rss/articles/"
            ]
            
            for feed_url in random.sample(tech_feeds, 2):
                try:
                    response = self.session.get(feed_url, timeout=15)
                    if response.status_code == 200:
                        feed = feedparser.parse(response.content)
                        
                        for entry in feed.entries[:5]:
                            title = entry.title
                            summary = entry.get('summary', '') or entry.get('description', '')
                            link = entry.link
                            
                            if self.is_relevant_content(title + summary):
                                image_url = self.extract_image_from_article(link) or self.get_relevant_image(title, summary)
                                
                                formatted_post = self.format_tech_post(title, summary)
                                
                                articles.append({
                                    'title': title,
                                    'summary': formatted_post,
                                    'category': 'technology',
                                    'url': link,
                                    'image_url': image_url,
                                    'found_date': datetime.now()
                                })
                                
                                if len(articles) >= 3:
                                    return articles
                    
                    time.sleep(2)
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–µ–Ω—Ç—ã {feed_url}: {e}")
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    def parse_historical_facts(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤"""
        try:
            articles = []
            
            historical_sources = [
                self.parse_historical_events_api,
                self.parse_wikipedia_today_in_history
            ]
            
            for source in historical_sources:
                try:
                    content = source()
                    if content:
                        articles.extend(content)
                        if len(articles) >= 2:
                            break
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {e}")
                    continue
            
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤: {e}")
            return []

    def parse_historical_events_api(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π —á–µ—Ä–µ–∑ API"""
        try:
            articles = []
            
            today = datetime.now()
            url = f"http://history.muffinlabs.com/date/{today.month}/{today.day}"
            
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                data = response.json()
                
                events = data.get('data', {}).get('Events', [])
                for event in events[:5]:
                    year = event.get('year', '')
                    text = event.get('text', '')
                    
                    if self.is_relevant_content(text):
                        image_url = self.get_historical_image_for_event(text, year)
                        
                        formatted_post = self.format_historical_post(f"–°–æ–±—ã—Ç–∏–µ {year} –≥–æ–¥–∞", text, year)
                        
                        articles.append({
                            'title': f"–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ —Å–æ–±—ã—Ç–∏–µ {year} –≥–æ–¥–∞",
                            'summary': formatted_post,
                            'category': 'history',
                            'url': data.get('url', ''),
                            'image_url': image_url,
                            'found_date': datetime.now()
                        })
                        
                        if len(articles) >= 2:
                            break
            
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π API: {e}")
            return []

    def parse_wikipedia_today_in_history(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ '—Å–µ–≥–æ–¥–Ω—è –≤ –∏—Å—Ç–æ—Ä–∏–∏' –∏–∑ Wikipedia"""
        try:
            articles = []
            
            today = datetime.now()
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'prop': 'extracts',
                'titles': f"{today.day} {self.get_month_name(today.month)}",
                'exintro': True,
                'explaintext': True,
                'format': 'json'
            }
            
            response = self.session.get(url, params=params, timeout=15)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                extract = page_data.get('extract', '')
                if extract and self.is_relevant_content(extract):
                    title = f"–°–æ–±—ã—Ç–∏—è {today.day} {self.get_month_name(today.month)}"
                    image_url = self.get_wikipedia_image(title)
                    
                    formatted_post = self.format_historical_post(title, extract[:500] + "...", "")
                    
                    articles.append({
                        'title': title,
                        'summary': formatted_post,
                        'category': 'history',
                        'url': f"https://ru.wikipedia.org/wiki/{urllib.parse.quote(title.replace(' ', '_'))}",
                        'image_url': image_url,
                        'found_date': datetime.now()
                    })
            
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Wikipedia —Å–µ–≥–æ–¥–Ω—è –≤ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            return []

    def parse_wikipedia_firsts(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–µ—Ä–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏"""
        try:
            articles = []
            
            search_queries = [
                "–ø–µ—Ä–≤—ã–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø—É—Ç–Ω–∏–∫", "–ø–µ—Ä–≤—ã–π –ø–æ–ª–µ—Ç –≤ –∫–æ—Å–º–æ—Å", 
                "–ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ", "—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ", "–ø–µ—Ä–≤–∞—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è",
                "–ø–µ—Ä–≤—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä", "–ø–µ—Ä–≤—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω", "–ø–µ—Ä–≤—ã–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å"
            ]
            
            for query in random.sample(search_queries, 3):
                try:
                    url = "https://ru.wikipedia.org/w/api.php"
                    params = {
                        'action': 'query',
                        'list': 'search',
                        'srsearch': query,
                        'format': 'json',
                        'srlimit': 3
                    }
                    
                    response = self.session.get(url, params=params, timeout=15)
                    data = response.json()
                    
                    for item in data.get('query', {}).get('search', []):
                        title = item.get('title', '')
                        
                        if self.is_relevant_content(title):
                            full_content = self.get_wikipedia_content(title)
                            if full_content:
                                image_url = self.get_wikipedia_image(title)
                                
                                formatted_post = self.format_wikipedia_post(title, full_content, query)
                                
                                articles.append({
                                    'title': title,
                                    'summary': formatted_post,
                                    'category': 'wikipedia',
                                    'url': f"https://ru.wikipedia.org/wiki/{urllib.parse.quote(title.replace(' ', '_'))}",
                                    'image_url': image_url,
                                    'found_date': datetime.now()
                                })
                                
                                if len(articles) >= 2:
                                    return articles
                    
                    time.sleep(1)
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞ {query}: {e}")
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Wikipedia: {e}")
            return []

    def get_month_name(self, month):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Å—è—Ü–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º"""
        months = {
            1: "—è–Ω–≤–∞—Ä—è", 2: "—Ñ–µ–≤—Ä–∞–ª—è", 3: "–º–∞—Ä—Ç–∞", 4: "–∞–ø—Ä–µ–ª—è",
            5: "–º–∞—è", 6: "–∏—é–Ω—è", 7: "–∏—é–ª—è", 8: "–∞–≤–≥—É—Å—Ç–∞", 
            9: "—Å–µ–Ω—Ç—è–±—Ä—è", 10: "–æ–∫—Ç—è–±—Ä—è", 11: "–Ω–æ—è–±—Ä—è", 12: "–¥–µ–∫–∞–±—Ä—è"
        }
        return months.get(month, "")

    def get_article_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏"""
        try:
            response = self.session.get(url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            for element in soup(['script', 'style', 'nav', 'header', 'footer']):
                element.decompose()
            
            content_selectors = [
                'article', '.article-content', '.post-content', 
                '.entry-content', '.content', 'main'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    text = content_elem.get_text(strip=True)
                    if len(text) > 200:
                        return text[:500] + '...'
            
            paragraphs = soup.find_all('p')
            text = ' '.join([p.get_text(strip=True) for p in paragraphs[:5]])
            return text[:500] + '...' if len(text) > 500 else text
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏: {e}")
            return ""

    def extract_image_from_article(self, url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        try:
            response = self.session.get(url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            image_selectors = [
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                '.article-image img',
                '.post-image img',
                '.entry-content img',
                'article img'
            ]
            
            for selector in image_selectors:
                img_elem = soup.select_one(selector)
                if img_elem:
                    if img_elem.get('content'):
                        image_url = img_elem['content']
                    else:
                        image_url = img_elem.get('src', '')
                    
                    if image_url and image_url.startswith('http'):
                        if image_url.startswith('//'):
                            image_url = 'https:' + image_url
                        elif image_url.startswith('/'):
                            from urllib.parse import urljoin
                            image_url = urljoin(url, image_url)
                        
                        return image_url
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None

    def get_wikipedia_content(self, title):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ Wikipedia"""
        try:
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'prop': 'extracts',
                'titles': title,
                'exintro': True,
                'explaintext': True,
                'format': 'json'
            }
            
            response = self.session.get(url, params=params, timeout=15)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                extract = page_data.get('extract', '')
                if extract:
                    paragraphs = [p for p in extract.split('\n') if p.strip()]
                    if paragraphs:
                        return paragraphs[0][:600] + '...' if len(paragraphs[0]) > 600 else paragraphs[0]
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ Wikipedia: {e}")
            return ""

    def get_wikipedia_image(self, title):
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ Wikipedia"""
        try:
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'prop': 'pageimages',
                'titles': title,
                'pithumbsize': 500,
                'format': 'json'
            }
            
            response = self.session.get(url, params=params, timeout=15)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                thumbnail = page_data.get('thumbnail')
                if thumbnail:
                    return thumbnail.get('source', '')
            
            return self.get_relevant_image(title, "")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è Wikipedia: {e}")
            return self.get_default_image()

    def get_relevant_image(self, title, content):
        """–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é"""
        try:
            text = (title + ' ' + content).lower()
            
            # –ö–æ—Å–º–æ—Å –∏ –∞—Å—Ç—Ä–æ–Ω–æ–º–∏—è
            if any(word in text for word in ['–∫–æ—Å–º–æ—Å', '—Å–ø—É—Ç–Ω–∏–∫', '—Ä–∞–∫–µ—Ç–∞', '–≥–∞–≥–∞—Ä–∏–Ω', '—Ç–µ—Ä–µ—à–∫–æ–≤–∞', '–∫–æ—Å–º–æ–Ω–∞–≤—Ç', '–æ—Ä–±–∏—Ç–∞', '–ø–ª–∞–Ω–µ—Ç–∞']):
                return self.get_space_image()
            
            # –ù–∞—É–∫–∞ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            elif any(word in text for word in ['–Ω–∞—É–∫–∞', '—É—á–µ–Ω—ã–π', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç']):
                return self.get_science_image()
            
            # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
            elif any(word in text for word in ['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '–∫–æ–º–ø—å—é—Ç–µ—Ä', '—Å–º–∞—Ä—Ç—Ñ–æ–Ω', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Å–æ—Ñ—Ç']):
                return self.get_tech_image()
            
            # –ò—Å—Ç–æ—Ä–∏—è
            elif any(word in text for word in ['–∏—Å—Ç–æ—Ä–∏—è', '–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π', '–¥—Ä–µ–≤–Ω–∏–π', '–∞—Ä—Ö–µ–æ–ª–æ–≥–∏—è', '–ø—Ä–æ—à–ª–æ–µ', '—Å–æ–±—ã—Ç–∏–µ']):
                return self.get_historical_image()
            
            # –ú–µ–¥–∏—Ü–∏–Ω–∞
            elif any(word in text for word in ['–º–µ–¥–∏—Ü–∏–Ω–∞', '–≤—Ä–∞—á', '–ª–µ–∫–∞—Ä—Å—Ç–≤–æ', '–±–æ–ª–µ–∑–Ω—å', '–∑–¥–æ—Ä–æ–≤—å–µ', '–≤–∏—Ä—É—Å']):
                return self.get_medical_image()
            
            # –ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è
            elif any(word in text for word in ['–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ', '–ø–∞—Ç–µ–Ω—Ç', '–∏–Ω–Ω–æ–≤–∞—Ü–∏—è', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '—Å–æ–∑–¥–∞–ª']):
                return self.get_invention_image()
            
            else:
                return self.get_thematic_image(text)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return self.get_default_image()

    def get_space_image(self):
        """–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–æ—Å–º–æ—Å–∞"""
        space_images = [
            'https://images.unsplash.com/photo-1446776653964-20c1d3a81b06?w=800&fit=crop',
            'https://images.unsplash.com/photo-1462331940025-496dfbfc7564?w=800&fit=crop',
            'https://images.unsplash.com/photo-1502136969935-8d8eef54d77b?w=800&fit=crop',
            'https://images.unsplash.com/photo-1464802686167-b939a6910659?w=800&fit=crop',
        ]
        return random.choice(space_images)

    def get_science_image(self):
        """–ù–∞—É—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        science_images = [
            'https://images.unsplash.com/photo-1532094349884-543bc11b234d?w=800&fit=crop',
            'https://images.unsplash.com/photo-1563089145-599997674d42?w=800&fit=crop',
            'https://images.unsplash.com/photo-1554475900-0a0350e3fc7b?w=800&fit=crop',
            'https://images.unsplash.com/photo-1507413245164-6160d8298b31?w=800&fit=crop',
        ]
        return random.choice(science_images)

    def get_tech_image(self):
        """–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        tech_images = [
            'https://images.unsplash.com/photo-1518709268805-4e9042af2176?w=800&fit=crop',
            'https://images.unsplash.com/photo-1517077304055-6e89abbf09b0?w=800&fit=crop',
            'https://images.unsplash.com/photo-1542831371-29b0f74f9713?w=800&fit=crop',
            'https://images.unsplash.com/photo-1550751827-4bd374c3f58b?w=800&fit=crop',
        ]
        return random.choice(tech_images)

    def get_historical_image(self):
        """–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        historical_images = [
            'https://images.unsplash.com/photo-1481627834876-b7833e8f5570?w=800&fit=crop',
            'https://images.unsplash.com/photo-1589652717521-10c0d092dea9?w=800&fit=crop',
            'https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=800&fit=crop',
            'https://images.unsplash.com/photo-1505664194779-8beaceb93744?w=800&fit=crop',
        ]
        return random.choice(historical_images)

    def get_medical_image(self):
        """–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        medical_images = [
            'https://images.unsplash.com/photo-1559757148-5c350d0d3c56?w=800&fit=crop',
            'https://images.unsplash.com/photo-1576091160399-112ba8d25d1f?w=800&fit=crop',
            'https://images.unsplash.com/photo-1559757175-0eb30cd8c063?w=800&fit=crop',
        ]
        return random.choice(medical_images)

    def get_invention_image(self):
        """–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π"""
        invention_images = [
            'https://images.unsplash.com/photo-1506744038136-46273834b3fb?w=800&fit=crop',
            'https://images.unsplash.com/photo-1558618666-fcd25c85cd64?w=800&fit=crop',
            'https://images.unsplash.com/photo-1454165804606-c3d57bc86b40?w=800&fit=crop',
        ]
        return random.choice(invention_images)

    def get_thematic_image(self, text):
        """–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞"""
        if any(word in text for word in ['–ø–µ—Ä–≤—ã–π', '—Ä–µ–∫–æ—Ä–¥', '–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ']):
            achievement_images = [
                'https://images.unsplash.com/photo-1563089145-599997674d42?w=800&fit=crop',
                'https://images.unsplash.com/photo-1517048676732-d65bc937f952?w=800&fit=crop',
            ]
            return random.choice(achievement_images)
        else:
            return self.get_default_image()

    def get_default_image(self):
        """–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        default_images = [
            'https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=800&fit=crop',
            'https://images.unsplash.com/photo-1481627834876-b7833e8f5570?w=800&fit=crop',
        ]
        return random.choice(default_images)

    def get_historical_image_for_event(self, event_text, year):
        """–°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–±—ã—Ç–∏—è"""
        text = event_text.lower()
        
        if any(word in text for word in ['–≤–æ–π–Ω–∞', '—Å—Ä–∞–∂–µ–Ω–∏–µ', '–±–∏—Ç–≤–∞']):
            return 'https://images.unsplash.com/photo-1547234931-12c622f4fc37?w=800&fit=crop'
        elif any(word in text for word in ['—Ä–µ–≤–æ–ª—é—Ü–∏—è', '–≤–æ—Å—Å—Ç–∞–Ω–∏–µ']):
            return 'https://images.unsplash.com/photo-1505664194779-8beaceb93744?w=800&fit=crop'
        else:
            return self.get_historical_image()

    def format_news_post(self, title, content, keyword):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ø–æ—Å—Ç"""
        templates = [
            "üì∞ –ê–ö–¢–£–ê–õ–¨–ù–ê–Ø –ù–û–í–û–°–¢–¨\n\n{title}\n\n{content}\n\nüîç –ù–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {keyword}\n\n#–Ω–æ–≤–æ—Å—Ç–∏ #–æ—Ç–∫—Ä—ã—Ç–∏–µ #–∞–∫—Ç—É–∞–ª—å–Ω–æ–µ",
            "üåç –í–ê–ñ–ù–û–ï –°–û–ë–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüîç –ù–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {keyword}\n\n#—Å–æ–±—ã—Ç–∏–µ #–≤–∞–∂–Ω–æ #–Ω–æ–≤–æ—Å—Ç–∏"
        ]
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content, keyword=keyword)

    def format_science_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—ã–π –ø–æ—Å—Ç"""
        templates = [
            "üî¨ –ù–ê–£–ß–ù–û–ï –û–¢–ö–†–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüí´ –≠—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∏–µ –º–µ–Ω—è–µ—Ç –Ω–∞—à–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ –º–∏—Ä–µ\n\n#–Ω–∞—É–∫–∞ #–æ—Ç–∫—Ä—ã—Ç–∏–µ #–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
            "üåå –ü–†–û–†–´–í –í –ù–ê–£–ö–ï\n\n{title}\n\n{content}\n\nüöÄ –£—á–µ–Ω—ã–µ —Å–æ–≤–µ—Ä—à–∏–ª–∏ –≤–∞–∂–Ω—ã–π —à–∞–≥ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏—Ä–æ–¥—ã\n\n#–Ω–∞—É–∫–∞ #–ø—Ä–æ—Ä—ã–≤ #–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"
        ]
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def format_tech_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Å—Ç"""
        templates = [
            "‚ö° –¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –†–ï–í–û–õ–Æ–¶–ò–Ø\n\n{title}\n\n{content}\n\nüí° –≠—Ç–æ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –º–µ–Ω—è–µ—Ç –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á\n\n#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ #–±—É–¥—É—â–µ–µ",
            "ü§ñ –ò–ù–ù–û–í–ê–¶–ò–û–ù–ù–ê–Ø –†–ê–ó–†–ê–ë–û–¢–ö–ê\n\n{title}\n\n{content}\n\nüöÄ –ü—Ä–æ–≥—Ä–µ—Å—Å –Ω–µ —Å—Ç–æ–∏—Ç –Ω–∞ –º–µ—Å—Ç–µ\n\n#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ #–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"
        ]
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def format_historical_post(self, title, content, year):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –ø–æ—Å—Ç"""
        templates = [
            "üèÜ –ò–°–¢–û–†–ò–ß–ï–°–ö–û–ï –°–û–ë–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüìÖ –ì–æ–¥: {year}\n\nüìö –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ —Å—Ç–∞–ª–æ –ø–æ–≤–æ—Ä–æ—Ç–Ω—ã–º –º–æ–º–µ–Ω—Ç–æ–º –≤ –∏—Å—Ç–æ—Ä–∏–∏\n\n#–∏—Å—Ç–æ—Ä–∏—è #—Å–æ–±—ã—Ç–∏–µ #–ø–∞–º—è—Ç—å",
            "üí° –í–ï–õ–ò–ö–û–ï –û–¢–ö–†–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüìÖ –ì–æ–¥: {year}\n\nüéØ –≠—Ç–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–∏–ª–æ —Ö–æ–¥ –∏—Å—Ç–æ—Ä–∏–∏\n\n#–∏—Å—Ç–æ—Ä–∏—è #–æ—Ç–∫—Ä—ã—Ç–∏–µ #–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ"
        ]
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content, year=year)

    def format_wikipedia_post(self, title, content, query):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç Wikipedia –ø–æ—Å—Ç"""
        templates = [
            "üåç –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–ô –§–ê–ö–¢\n\n{title}\n\n{content}\n\nüîç –ù–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}\n\nüìö –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ —Å—Ç–∞–ª–æ –≤–∞–∂–Ω–æ–π –≤–µ—Ö–æ–π\n\n#–∏—Å—Ç–æ—Ä–∏—è #—Ñ–∞–∫—Ç #–ø–∞–º—è—Ç—å",
            "üí´ –ü–ï–†–í–´–ô –®–ê–ì\n\n{title}\n\n{content}\n\nüîç –ù–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}\n\nüöÄ –≠—Ç–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã–ª–æ –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏\n\n#–ø–µ—Ä–≤—ã–π #–∏—Å—Ç–æ—Ä–∏—è #–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ"
        ]
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content, query=query)

    def is_relevant_content(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        keywords = [
            '–ø–µ—Ä–≤—ã–π', '–ø–µ—Ä–≤–æ–µ', '–ø–µ—Ä–≤–∞—è', '–≤–ø–µ—Ä–≤—ã–µ', '–≤–ø–µ—Ä–≤—ã',
            '–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '—Ä–µ–≤–æ–ª—é—Ü–∏—è', '–ø—Ä–æ—Ä—ã–≤',
            '—Ä–µ–∫–æ—Ä–¥', '–∏—Å—Ç–æ—Ä–∏—è', '—Å–æ–∑–¥–∞–Ω', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω', '–∑–∞–ø—É—â–µ–Ω',
            '–æ–±–Ω–∞—Ä—É–∂–µ–Ω', '–Ω–∞—á–∞–ª–æ', '–≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ', '–ø–æ—è–≤–ª–µ–Ω–∏–µ',
            '–æ—Å–Ω–æ–≤–∞–Ω–∏–µ', '—Å–æ–∑–¥–∞–Ω–∏–µ', '–∏–∑–æ–±—Ä–µ—Ç', '–æ—Ç–∫—Ä—ã—Ç', '–Ω–æ–≤–∞—è —ç—Ä–∞',
            '–ø–µ—Ä–µ–ª–æ–º–Ω—ã–π –º–æ–º–µ–Ω—Ç', '–∑–Ω–∞–∫–æ–≤–æ–µ', '—ç–ø–æ—Ö–∞–ª—å–Ω–æ–µ', '–ø–∏–æ–Ω–µ—Ä',
            '–ø–µ—Ä–≤–æ–æ—Ç–∫—Ä—ã–≤–∞—Ç–µ–ª—å', '–Ω–æ–≤–∞—Ç–æ—Ä', '–∏–Ω–Ω–æ–≤–∞—Ü'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in keywords)

    def format_for_preview(self, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
        current_time = datetime.now()
        preview_text = f"üì∞ –ù–û–í–´–ô –ü–û–°–¢ –î–õ–Ø –ü–£–ë–õ–ò–ö–ê–¶–ò–ò\n\n{content['summary']}\n\n"
        
        if content.get('image_url'):
            preview_text += f"üñºÔ∏è –ü—Ä–∏–ª–æ–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n\n"
        
        preview_text += f"‚è∞ –ù–∞–π–¥–µ–Ω–æ: {current_time.strftime('%H:%M %d.%m.%Y')}"
        
        return preview_text

def setup_content_finder(db_manager=None):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–µ—Ä–µ–¥–∞—á–µ–π db_manager"""
    return ContentFinder(db_manager)
