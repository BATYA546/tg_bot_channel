# content_finder.py
import requests
import logging
from datetime import datetime
import time
from bs4 import BeautifulSoup
import random
import re

logger = logging.getLogger(__name__)

class ContentFinder:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        self.keywords = [
            "–ø–µ—Ä–≤", "—Ä–µ–≤–æ–ª—é—Ü", "—Ä–µ–∫–æ—Ä–¥", "–ø—Ä–æ—Ä—ã–≤", "–∏–Ω–Ω–æ–≤–∞—Ü", "–æ—Ç–∫—Ä—ã—Ç–∏–µ",
            "–∏–∑–æ–±—Ä–µ—Ç", "—Ç–µ—Ö–Ω–æ–ª–æ–≥", "–Ω–∞—É–∫", "–∏—Å—Ç–æ—Ä–∏", "—É–Ω–∏–∫–∞–ª—å–Ω", "–Ω–æ–≤—ã–π",
            "–ø—Ä–æ–≥—Ä–µ—Å—Å", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫", "—Å–æ–∑–¥–∞–Ω", "–∑–∞–ø—É—Å–∫", "–æ–±–Ω–∞—Ä—É–∂–µ–Ω"
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.sources = [
            {
                'name': '–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏ (–Ω–∞—É–∫–∞)',
                'url': 'https://ria.ru/science/',
                'parser': self.parse_ria_science
            },
            {
                'name': '–¢–ê–°–° (–Ω–∞—É–∫–∞)',
                'url': 'https://tass.ru/nauka',
                'parser': self.parse_tass_science
            },
            {
                'name': 'N+1 (–∞—Ä—Ö–∏–≤)',
                'url': 'https://nplus1.ru/news/archive',
                'parser': self.parse_nplus1_archive
            }
        ]

    def search_content(self, max_posts=5):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        found_content = []
        
        for source in self.sources:
            try:
                logger.info(f"üì∞ –ü—Ä–æ–≤–µ—Ä—è—é {source['name']}...")
                content = source['parser'](source['url'])
                if content:
                    found_content.extend(content)
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(content)} –∑–∞–ø–∏—Å–µ–π")
                
                if len(found_content) >= max_posts:
                    found_content = found_content[:max_posts]
                    break
                    
                time.sleep(3)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source['name']}: {e}")
                continue
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback - —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        if not found_content:
            found_content = self.generate_fallback_content()
        
        logger.info(f"üéØ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {len(found_content)}")
        return found_content

    def parse_ria_science(self, url):
        """–ü–∞—Ä—Å–µ—Ä –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏ (–Ω–∞—É–∫–∞)"""
        try:
            response = self.session.get(url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = []
            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
            news_items = soup.find_all('div', class_=['list-item', 'cell'])
            
            for item in news_items[:15]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                try:
                    title_elem = item.find(['a', 'h2', 'span'])
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text().strip()
                    if len(title) < 10:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        continue
                    
                    # –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    if self.is_relevant_content(title):
                        # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É
                        link = title_elem.get('href', '')
                        if link and not link.startswith('http'):
                            link = 'https://ria.ru' + link
                            
                        article_data = {
                            'title': title,
                            'summary': self.generate_summary(title),
                            'source': '–†–ò–ê –ù–∞—É–∫–∞',
                            'url': link or 'https://ria.ru/science/',
                            'category': self.categorize_content(title),
                            'found_date': datetime.now()
                        }
                        articles.append(article_data)
                        
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏ –†–ò–ê: {e}")
                    continue
                    
            return articles[:3]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ –±–æ–ª–µ–µ 3 —Å—Ç–∞—Ç–µ–π
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –†–ò–ê: {e}")
            return []

    def parse_tass_science(self, url):
        """–ü–∞—Ä—Å–µ—Ä –¢–ê–°–° (–Ω–∞—É–∫–∞)"""
        try:
            response = self.session.get(url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = []
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –¢–ê–°–°
            news_items = soup.find_all(['article', 'div'], class_=True)
            
            for item in news_items[:20]:
                try:
                    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ–≥–∞—Ö
                    title_elem = (item.find('h2') or 
                                 item.find('h3') or 
                                 item.find('a') or 
                                 item.find('span'))
                    
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text().strip()
                    if len(title) < 15:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        continue
                    
                    if self.is_relevant_content(title):
                        article_data = {
                            'title': title,
                            'summary': self.generate_summary(title),
                            'source': '–¢–ê–°–° –ù–∞—É–∫–∞',
                            'url': url,
                            'category': self.categorize_content(title),
                            'found_date': datetime.now()
                        }
                        articles.append(article_data)
                        
                except Exception as e:
                    continue
                    
            return articles[:2]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¢–ê–°–°: {e}")
            return []

    def parse_nplus1_archive(self, url):
        """–ü–∞—Ä—Å–µ—Ä –∞—Ä—Ö–∏–≤–∞ N+1"""
        try:
            response = self.session.get(url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = []
            news_items = soup.find_all('article')[:10]
            
            for item in news_items:
                try:
                    title_elem = item.find('h2') or item.find('a')
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text().strip()
                    
                    if self.is_relevant_content(title):
                        article_data = {
                            'title': title,
                            'summary': self.generate_summary(title),
                            'source': 'N+1',
                            'url': 'https://nplus1.ru',
                            'category': self.categorize_content(title),
                            'found_date': datetime.now()
                        }
                        articles.append(article_data)
                        
                except Exception:
                    continue
                    
            return articles[:2]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ N+1: {e}")
            return []

    def is_relevant_content(self, text):
        """–ë–æ–ª–µ–µ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
        for keyword in self.keywords:
            if keyword in text_lower:
                return True
                
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        science_words = ['—É—á–µ–Ω', '–∏—Å—Å–ª–µ–¥–æ–≤–∞', '—Ä–∞–∑—Ä–∞–±–æ—Ç', '—Å–æ–∑–¥–∞', '–æ–±–Ω–∞—Ä—É–∂']
        for word in science_words:
            if word in text_lower:
                return True
                
        return False

    def generate_summary(self, title):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        summaries = [
            f"–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π: {title}",
            f"–ù–æ–≤–æ–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π: {title}",
            f"–ü—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π: {title}",
            f"–í–∞–∂–Ω–æ–µ –Ω–∞—É—á–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ: {title}",
            f"–ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞: {title}"
        ]
        return random.choice(summaries)

    def categorize_content(self, title):
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['—Ç–µ—Ö–Ω–æ–ª–æ–≥', '–∏–Ω–Ω–æ–≤–∞—Ü', '–∏–∑–æ–±—Ä–µ—Ç', '—Ä–∞–∑—Ä–∞–±–æ—Ç']):
            return 'technology'
        elif any(word in title_lower for word in ['–Ω–∞—É–∫', '–æ—Ç–∫—Ä—ã—Ç', '–∏—Å—Å–ª–µ–¥–æ–≤–∞', '—É—á–µ–Ω']):
            return 'science'
        elif any(word in title_lower for word in ['—Ä–µ–∫–æ—Ä–¥', '–ø–µ—Ä–≤—ã–π', '–∏—Å—Ç–æ—Ä–∏', '—É–Ω–∏–∫–∞–ª—å–Ω']):
            return 'records'
        else:
            return 'discovery'

    def generate_fallback_content(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"""
        logger.info("üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç...")
        
        fallback_titles = [
            "–£—á–µ–Ω—ã–µ —Å–æ–∑–¥–∞–ª–∏ –ø–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ –∫–≤–∞–Ω—Ç–æ–≤—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä —Å —Ä–µ–∫–æ—Ä–¥–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é",
            "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –ø–æ–∑–≤–æ–ª–∏–ª–∞ –≤–ø–µ—Ä–≤—ã–µ –ø–æ–ª—É—á–∏—Ç—å —ç–Ω–µ—Ä–≥–∏—é –∏–∑ –≤–∞–∫—É—É–º–∞",
            "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —á–∞—Å—Ç–∏—Ü–∞, –º–µ–Ω—è—é—â–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ —Ñ–∏–∑–∏–∫–µ",
            "–ü–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –ø—Ä–æ—à–µ–ª —Ç–µ—Å—Ç –¢—å—é—Ä–∏–Ω–≥–∞ —Å —Ä–µ–∫–æ—Ä–¥–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º",
            "–ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞: —Å–æ–∑–¥–∞–Ω—ã —Å–æ–ª–Ω–µ—á–Ω—ã–µ –±–∞—Ç–∞—Ä–µ–∏ —Å –ö–ü–î –±–æ–ª–µ–µ 50%"
        ]
        
        content_list = []
        for title in random.sample(fallback_titles, 3):
            content_list.append({
                'title': title,
                'summary': self.generate_summary(title),
                'source': '–ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è',
                'url': '',
                'category': self.categorize_content(title),
                'found_date': datetime.now()
            })
        
        return content_list

# –í –º–µ—Ç–æ–¥–µ format_for_preview –º–µ–Ω—è–µ–º:
def format_for_preview(self, content):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
    # –°–æ–∑–¥–∞–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    category_emojis = {
        'technology': 'üîß',
        'science': 'üî¨', 
        'records': 'üèÜ',
        'discovery': 'üí°'
    }
    
    emoji = category_emojis.get(content['category'], 'üì∞')
    
    return f"""
{emoji} *–ù–æ–≤—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏*

*–ó–∞–≥–æ–ª–æ–≤–æ–∫:* {content['title']}
*–ö–∞—Ç–µ–≥–æ—Ä–∏—è:* {content['category']}

*–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:*
{content['summary']}

‚è∞ –ù–∞–π–¥–µ–Ω–æ: {content['found_date'].strftime('%H:%M %d.%m.%Y')}
        """

def setup_content_finder():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    return ContentFinder()
