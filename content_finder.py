# content_finder.py
import requests
import logging
from datetime import datetime
import time
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class ContentFinder:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.keywords = [
            "–ø–µ—Ä–≤–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ", "—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è", "–º–∏—Ä–æ–≤–æ–π —Ä–µ–∫–æ—Ä–¥",
            "–≤–ø–µ—Ä–≤—ã–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏", "–ø—Ä–æ—Ä—ã–≤ –≤ –Ω–∞—É–∫–µ", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ",
            "–Ω–æ–≤—ã–π —Ä–µ–∫–æ—Ä–¥", "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ —Å–æ–±—ã—Ç–∏–µ", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ä—ã–≤",
            "–Ω–∞—É—á–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ", "–ø–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ", "—É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ"
        ]
        
        self.sources = [
            {
                'name': '–ù–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏',
                'url': 'https://nplus1.ru/news',
                'parser': self.parse_nplus1
            },
        ]

    def search_content(self, max_posts=3):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        found_content = []
        
        for source in self.sources:
            try:
                logger.info(f"üì∞ –ü—Ä–æ–≤–µ—Ä—è—é {source['name']}...")
                content = source['parser']()
                if content:
                    found_content.extend(content)
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(content)} –∑–∞–ø–∏—Å–µ–π")
                
                if len(found_content) >= max_posts:
                    found_content = found_content[:max_posts]
                    break
                    
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source['name']}: {e}")
                continue
        
        logger.info(f"üéØ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {len(found_content)}")
        return found_content

    def parse_nplus1(self):
        """–ü–∞—Ä—Å–µ—Ä –Ω–∞—É—á–Ω–æ–≥–æ —Å–∞–π—Ç–∞ nplus1.ru"""
        try:
            response = self.session.get(self.sources[0]['url'], timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = []
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
            news_items = soup.find_all('article', class_='news')[:5]
            
            for item in news_items:
                try:
                    title_elem = item.find('h2') or item.find('a')
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text().strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                    if self.is_relevant_content(title):
                        article_data = {
                            'title': title,
                            'summary': title,  # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ —Ç–µ–∫—Å—Ç
                            'source': 'N+1',
                            'url': 'https://nplus1.ru',
                            'category': self.categorize_content(title),
                            'found_date': datetime.now()
                        }
                        articles.append(article_data)
                        
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ N+1: {e}")
            return []

    def is_relevant_content(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.keywords)

    def categorize_content(self, title):
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['—Ç–µ—Ö–Ω–æ–ª–æ–≥', '–∏–Ω–Ω–æ–≤–∞—Ü', '–∏–∑–æ–±—Ä–µ—Ç']):
            return 'technology'
        elif any(word in title_lower for word in ['–Ω–∞—É–∫', '–æ—Ç–∫—Ä—ã—Ç', '–∏—Å—Å–ª–µ–¥–æ–≤–∞']):
            return 'science'
        elif any(word in title_lower for word in ['—Ä–µ–∫–æ—Ä–¥', '–ø–µ—Ä–≤—ã–π', '–∏—Å—Ç–æ—Ä–∏']):
            return 'records'
        else:
            return 'general'

    def format_for_preview(self, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
        return f"""
üì∞ *–ù–æ–≤—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏*

*–ó–∞–≥–æ–ª–æ–≤–æ–∫:* {content['title']}
*–ö–∞—Ç–µ–≥–æ—Ä–∏—è:* {content['category']}
*–ò—Å—Ç–æ—á–Ω–∏–∫:* {content['source']}

*–¢–µ–∫—Å—Ç:*
{content['summary'][:200]}...

‚è∞ –ù–∞–π–¥–µ–Ω–æ: {content['found_date'].strftime('%H:%M %d.%m.%Y')}
        """

def setup_content_finder():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    return ContentFinder()
