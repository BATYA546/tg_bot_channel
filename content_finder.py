# content_finder.py
import logging
import requests
from datetime import datetime
import random
import hashlib
from bs4 import BeautifulSoup
import re
import json
import time

logger = logging.getLogger(__name__)

class ContentFinder:
    def __init__(self, db_manager=None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        self.db_manager = db_manager
        self.post_hashes = set()
        self.load_existing_hashes()
        
        self.sources = [
            self.parse_wikipedia_firsts,
            self.parse_historical_firsts,
            self.parse_science_firsts,
            self.parse_tech_firsts
        ]

    def load_existing_hashes(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ö–µ—à–∏ –∏–∑ –ë–î –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if self.db_manager:
            try:
                conn = self.db_manager.get_connection()
                cursor = conn.cursor()
                cursor.execute('SELECT title, content FROM found_content')
                existing_posts = cursor.fetchall()
                
                for title, content in existing_posts:
                    text = title + content
                    content_hash = hashlib.md5(text.encode()).hexdigest()
                    self.post_hashes.add(content_hash)
                    
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.post_hashes)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ö–µ—à–µ–π –∏–∑ –ë–î")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ö–µ—à–µ–π –∏–∑ –ë–î: {e}")

    def search_content(self, max_posts=3):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        found_content = []
        attempts = 0
        max_attempts = 8  # –£–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        
        while len(found_content) < max_posts and attempts < max_attempts:
            attempts += 1
            logger.info(f"üîç –ü–æ–ø—ã—Ç–∫–∞ {attempts}/{max_attempts}")
            
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            random.shuffle(self.sources)
            
            for source in self.sources:
                try:
                    if len(found_content) >= max_posts:
                        break
                        
                    content_list = source()
                    if content_list:
                        for content in content_list:
                            if self.is_truly_unique_content(content) and len(found_content) < max_posts:
                                found_content.append(content)
                                content_hash = self.get_content_hash(content)
                                self.post_hashes.add(content_hash)
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç: {content['title'][:50]}...")
                            
                            if len(found_content) >= max_posts:
                                break
                
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source.__name__}: {e}")
                    continue
                
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
            time.sleep(1)
        
        logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {len(found_content)}")
        return found_content

    def is_truly_unique_content(self, content):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ —Ö–µ—à–∏ –∏ –ë–î"""
        content_hash = self.get_content_hash(content)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ø–∞–º—è—Ç–∏
        if content_hash in self.post_hashes:
            logger.info(f"üö´ –ü–æ—Å—Ç —É–∂–µ –≤ –ø–∞–º—è—Ç–∏: {content['title'][:30]}...")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ë–î
        if self.db_manager and self.is_content_in_db(content):
            logger.info(f"üö´ –ü–æ—Å—Ç —É–∂–µ –≤ –ë–î: {content['title'][:30]}...")
            return False
            
        return True

    def is_content_in_db(self, content):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = self.db_manager.get_connection()
            cursor = conn.cursor()
            
            # –ë–æ–ª–µ–µ –≥–∏–±–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –∏—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            cursor.execute('''
                SELECT id FROM found_content 
                WHERE title LIKE %s OR content LIKE %s
            ''', (f"%{content['title'][:30]}%", f"%{content['title'][:20]}%"))
            
            result = cursor.fetchone()
            return result is not None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ë–î: {e}")
            return False

    def get_content_hash(self, content):
        """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        text = content['title'] + content['summary'][:100]
        return hashlib.md5(text.encode()).hexdigest()

    def parse_wikipedia_firsts(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–µ—Ä–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏"""
        try:
            articles = []
            
            # –°–ø–∏—Å–æ–∫ —Ç–µ–º –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π
            search_queries = [
                "–ø–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ",
                "–ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ", 
                "–ø–µ—Ä–≤–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ",
                "–ø–µ—Ä–≤—ã–π –ø–æ–ª–µ—Ç",
                "–ø–µ—Ä–≤–∞—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è",
                "–ø–µ—Ä–≤—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä",
                "–ø–µ—Ä–≤—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω",
                "–ø–µ—Ä–≤—ã–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å",
                "–ø–µ—Ä–≤—ã–π —Ñ–∏–ª—å–º",
                "–ø–µ—Ä–≤–∞—è –∫–Ω–∏–≥–∞"
            ]
            
            for query in random.sample(search_queries, 3):
                try:
                    url = "https://ru.wikipedia.org/w/api.php"
                    params = {
                        'action': 'query',
                        'list': 'search',
                        'srsearch': query,
                        'format': 'json',
                        'srlimit': 3
                    }
                    
                    response = self.session.get(url, params=params, timeout=10)
                    data = response.json()
                    
                    for item in data.get('query', {}).get('search', []):
                        title = item.get('title', '')
                        snippet = item.get('snippet', '')
                        
                        # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –∏–∑ —Å–Ω–∏–ø–ø–µ—Ç–∞
                        soup = BeautifulSoup(snippet, 'html.parser')
                        clean_snippet = soup.get_text()
                        
                        if self.is_relevant_content(title + clean_snippet):
                            full_content = self.get_wikipedia_content(title)
                            if full_content and len(full_content) > 50:
                                formatted_post = self.format_wikipedia_post(title, full_content)
                                
                                articles.append({
                                    'title': title,
                                    'summary': formatted_post,
                                    'category': 'history',
                                    'url': f"https://ru.wikipedia.org/wiki/{title.replace(' ', '_')}",
                                    'image_url': self.get_wikipedia_image(title),
                                    'found_date': datetime.now()
                                })
                                
                                if len(articles) >= 2:
                                    break
                    
                    time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞ {query}: {e}")
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Wikipedia: {e}")
            return []

    def parse_historical_firsts(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –ø–µ—Ä–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ API –∏ —Å–∞–π—Ç—ã
            articles = []
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
            sources = [
                self.parse_russian_history,
                self.parse_science_history
            ]
            
            for source in sources:
                try:
                    content = source()
                    if content:
                        articles.extend(content)
                        if len(articles) >= 2:
                            break
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
                    continue
            
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π: {e}")
            return []

    def parse_russian_history(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä—É—Å—Å–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏ - –ø–µ—Ä–≤—ã–µ —Å–æ–±—ã—Ç–∏—è"""
        try:
            # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è –†–æ—Å—Å–∏–∏
            url = "https://histrf.ru/read/articles"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = []
            
            # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –ø–µ—Ä–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π
            news_items = soup.find_all('article', class_=re.compile('article|news|item'))[:5]
            
            for item in news_items:
                try:
                    title_elem = item.find('h2') or item.find('h3') or item.find('a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text().strip()
                    
                    if self.is_relevant_content(title):
                        # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                        desc_elem = item.find('p') or item.find('div', class_=re.compile('desc|text|content'))
                        description = desc_elem.get_text().strip() if desc_elem else ""
                        
                        formatted_post = self.format_historical_post(title, description)
                        
                        articles.append({
                            'title': title,
                            'summary': formatted_post,
                            'category': 'history',
                            'url': "https://histrf.ru",
                            'image_url': self.get_historical_image(),
                            'found_date': datetime.now()
                        })
                        
                except Exception as e:
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä—É—Å—Å–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            return []

    def parse_science_firsts(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞—É—á–Ω—ã—Ö –ø–µ—Ä–≤—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π"""
        try:
            articles = []
            
            # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ —Å–∞–π—Ç—ã
            science_sources = [
                "https://naked-science.ru",
                "https://elementy.ru",
                "https://scientificrussia.ru"
            ]
            
            for source_url in random.sample(science_sources, 2):
                try:
                    response = self.session.get(source_url, timeout=10)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏
                    items = soup.find_all('article')[:3] or soup.find_all('div', class_=re.compile('article|news|post'))[:3]
                    
                    for item in items:
                        try:
                            title_elem = item.find('h2') or item.find('h3') or item.find('a')
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text().strip()
                            
                            if self.is_relevant_content(title):
                                desc_elem = item.find('p') or item.find('div', class_=re.compile('desc|text|excerpt'))
                                description = desc_elem.get_text().strip() if desc_elem else ""
                                
                                formatted_post = self.format_science_post(title, description)
                                
                                articles.append({
                                    'title': title,
                                    'summary': formatted_post,
                                    'category': 'science',
                                    'url': source_url,
                                    'image_url': self.get_science_image(),
                                    'found_date': datetime.now()
                                })
                                
                                if len(articles) >= 2:
                                    break
                                    
                        except Exception as e:
                            continue
                            
                    time.sleep(1)
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source_url}: {e}")
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    def parse_tech_firsts(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–µ—Ä–≤—ã—Ö –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π"""
        try:
            articles = []
            
            # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–∞–π—Ç—ã
            tech_sources = [
                "https://3dnews.ru",
                "https://hi-news.ru",
                "https://www.popmech.ru"
            ]
            
            for source_url in random.sample(tech_sources, 2):
                try:
                    response = self.session.get(source_url, timeout=10)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # –ò—â–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
                    items = soup.find_all('article')[:3] or soup.find_all('div', class_=re.compile('news|item|post'))[:3]
                    
                    for item in items:
                        try:
                            title_elem = item.find('h2') or item.find('h3') or item.find('a')
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text().strip()
                            
                            if self.is_relevant_content(title):
                                desc_elem = item.find('p') or item.find('div', class_=re.compile('desc|text|excerpt'))
                                description = desc_elem.get_text().strip() if desc_elem else ""
                                
                                formatted_post = self.format_tech_post(title, description)
                                
                                articles.append({
                                    'title': title,
                                    'summary': formatted_post,
                                    'category': 'technology',
                                    'url': source_url,
                                    'image_url': self.get_tech_image(),
                                    'found_date': datetime.now()
                                })
                                
                                if len(articles) >= 2:
                                    break
                                    
                        except Exception as e:
                            continue
                            
                    time.sleep(1)
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source_url}: {e}")
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    def parse_science_history(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—Ä–∏–∏ –Ω–∞—É–∫–∏"""
        try:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤
            articles = []
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤
            historical_facts = [
                {
                    'title': '–ü–µ—Ä–≤—ã–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø—É—Ç–Ω–∏–∫ –ó–µ–º–ª–∏',
                    'content': '–ó–∞–ø—É—Å–∫ –ø–µ—Ä–≤–æ–≥–æ –≤ –º–∏—Ä–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–ø—É—Ç–Ω–∏–∫–∞ –ó–µ–º–ª–∏ —Å–æ—Å—Ç–æ—è–ª—Å—è 4 –æ–∫—Ç—è–±—Ä—è 1957 –≥–æ–¥–∞. –°–ø—É—Ç–Ω–∏–∫ –ü–°-1 –±—ã–ª –∑–∞–ø—É—â–µ–Ω —Å –∫–æ—Å–º–æ–¥—Ä–æ–º–∞ –ë–∞–π–∫–æ–Ω—É—Ä –∏ –æ—Ç–∫—Ä—ã–ª –∫–æ—Å–º–∏—á–µ—Å–∫—É—é —ç—Ä—É —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞.',
                    'category': 'space'
                },
                {
                    'title': '–ü–µ—Ä–≤–∞—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –≤ –∏—Å—Ç–æ—Ä–∏–∏',
                    'content': '–ü–µ—Ä–≤–∞—è –≤ –∏—Å—Ç–æ—Ä–∏–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –±—ã–ª–∞ —Å–¥–µ–ª–∞–Ω–∞ –ñ–æ–∑–µ—Ñ–æ–º –ù–∏—Å–µ—Ñ–æ—Ä–æ–º –ù—å–µ–ø—Å–æ–º –≤ 1826 –≥–æ–¥—É. –°–Ω–∏–º–æ–∫ ¬´–í–∏–¥ –∏–∑ –æ–∫–Ω–∞ –≤ –õ–µ –ì—Ä–∞¬ª —Å–æ–∑–¥–∞–≤–∞–ª—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ 8 —á–∞—Å–æ–≤ —ç–∫—Å–ø–æ–∑–∏—Ü–∏–∏.',
                    'category': 'photography'
                },
                {
                    'title': '–ü–µ—Ä–≤—ã–π –ø–æ–ª–µ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –≤ –∫–æ—Å–º–æ—Å',
                    'content': '12 –∞–ø—Ä–µ–ª—è 1961 –≥–æ–¥–∞ –Æ—Ä–∏–π –ì–∞–≥–∞—Ä–∏–Ω —Å—Ç–∞–ª –ø–µ—Ä–≤—ã–º —á–µ–ª–æ–≤–µ–∫–æ–º, —Å–æ–≤–µ—Ä—à–∏–≤—à–∏–º –ø–æ–ª–µ—Ç –≤ –∫–æ—Å–º–æ—Å –Ω–∞ –∫–æ—Ä–∞–±–ª–µ ¬´–í–æ—Å—Ç–æ–∫-1¬ª. –ü–æ–ª–µ—Ç –¥–ª–∏–ª—Å—è 108 –º–∏–Ω—É—Ç.',
                    'category': 'space'
                },
                {
                    'title': '–ü–µ—Ä–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–Ω–µ—Å—Ç–µ–∑–∏–∏',
                    'content': '16 –æ–∫—Ç—è–±—Ä—è 1846 –≥–æ–¥–∞ –£–∏–ª—å—è–º –ú–æ—Ä—Ç–æ–Ω –≤–ø–µ—Ä–≤—ã–µ –ø—É–±–ª–∏—á–Ω–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ñ–∏—Ä–Ω–æ–π –∞–Ω–µ—Å—Ç–µ–∑–∏–∏ –≤–æ –≤—Ä–µ–º—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –ú–∞—Å—Å–∞—á—É—Å–µ—Ç—Å–∫–æ–π –±–æ–ª—å–Ω–∏—Ü–µ.',
                    'category': 'medicine'
                },
                {
                    'title': '–ü–µ—Ä–≤—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä',
                    'content': '10 –º–∞—Ä—Ç–∞ 1876 –≥–æ–¥–∞ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ë–µ–ª–ª –ø—Ä–æ–∏–∑–Ω–µ—Å –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É: ¬´–ú–∏—Å—Ç–µ—Ä –í–∞—Ç—Å–æ–Ω, –∏–¥–∏—Ç–µ —Å—é–¥–∞, –≤—ã –º–Ω–µ –Ω—É–∂–Ω—ã¬ª.',
                    'category': 'technology'
                }
            ]
            
            for fact in random.sample(historical_facts, 2):
                formatted_post = self.format_historical_post(fact['title'], fact['content'])
                
                articles.append({
                    'title': fact['title'],
                    'summary': formatted_post,
                    'category': fact['category'],
                    'url': '',
                    'image_url': self.get_category_image(fact['category']),
                    'found_date': datetime.now()
                })
            
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return []

    def get_wikipedia_content(self, title):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ Wikipedia"""
        try:
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'prop': 'extracts',
                'titles': title,
                'exintro': True,
                'explaintext': True,
                'format': 'json'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                extract = page_data.get('extract', '')
                if extract:
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü
                    first_para = extract.split('\n')[0]
                    return first_para[:500] + '...' if len(first_para) > 500 else first_para
            
            return ""
            
        except Exception as e:
            return ""

    def get_wikipedia_image(self, title):
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ Wikipedia"""
        try:
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'prop': 'pageimages',
                'titles': title,
                'pithumbsize': 500,
                'format': 'json'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                thumbnail = page_data.get('thumbnail')
                if thumbnail:
                    return thumbnail.get('source', '')
            
            return self.get_category_image('history')
            
        except Exception as e:
            return self.get_category_image('history')

    def format_wikipedia_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç Wikipedia –ø–æ—Å—Ç"""
        templates = [
            "üåç –ò–°–¢–û–†–ò–ß–ï–°–ö–û–ï –°–û–ë–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüìö –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ —Å—Ç–∞–ª–æ –≤–∞–∂–Ω–æ–π –≤–µ—Ö–æ–π –≤ –∏—Å—Ç–æ—Ä–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞.\n\n#–∏—Å—Ç–æ—Ä–∏—è #—Å–æ–±—ã—Ç–∏–µ #–ø–∞–º—è—Ç—å",
            
            "üí´ –ü–ï–†–í–´–ô –®–ê–ì\n\n{title}\n\n{content}\n\nüöÄ –≠—Ç–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã–ª–æ –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏.\n\n#–ø–µ—Ä–≤—ã–π #–∏—Å—Ç–æ—Ä–∏—è #–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ"
        ]
        
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def format_science_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—ã–π –ø–æ—Å—Ç"""
        templates = [
            "üî¨ –ù–ê–£–ß–ù–û–ï –û–¢–ö–†–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüí´ –≠—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∏–µ –∏–∑–º–µ–Ω–∏–ª–æ –Ω–∞—à–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ –º–∏—Ä–µ.\n\n#–Ω–∞—É–∫–∞ #–æ—Ç–∫—Ä—ã—Ç–∏–µ #–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
            
            "üåå –ü–†–û–†–´–í –í –ù–ê–£–ö–ï\n\n{title}\n\n{content}\n\nüöÄ –£—á–µ–Ω—ã–µ —Å–æ–≤–µ—Ä—à–∏–ª–∏ –≤–∞–∂–Ω—ã–π —à–∞–≥ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏—Ä–æ–¥—ã.\n\n#–Ω–∞—É–∫–∞ #–ø—Ä–æ—Ä—ã–≤ #–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"
        ]
        
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def format_tech_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Å—Ç"""
        templates = [
            "‚ö° –¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –†–ï–í–û–õ–Æ–¶–ò–Ø\n\n{title}\n\n{content}\n\nüí° –≠—Ç–æ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –∏–∑–º–µ–Ω–∏–ª–æ –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á.\n\n#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ #–±—É–¥—É—â–µ–µ",
            
            "ü§ñ –ò–ù–ù–û–í–ê–¶–ò–û–ù–ù–ê–Ø –†–ê–ó–†–ê–ë–û–¢–ö–ê\n\n{title}\n\n{content}\n\nüöÄ –ü—Ä–æ–≥—Ä–µ—Å—Å –Ω–µ —Å—Ç–æ–∏—Ç –Ω–∞ –º–µ—Å—Ç–µ - —ç—Ç–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã.\n\n#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ #–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"
        ]
        
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def format_historical_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –ø–æ—Å—Ç"""
        templates = [
            "üèÜ –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–ô –§–ê–ö–¢\n\n{title}\n\n{content}\n\nüìö –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ —Å—Ç–∞–ª–æ –ø–æ–≤–æ—Ä–æ—Ç–Ω—ã–º –º–æ–º–µ–Ω—Ç–æ–º –≤ –∏—Å—Ç–æ—Ä–∏–∏.\n\n#–∏—Å—Ç–æ—Ä–∏—è #—Ñ–∞–∫—Ç #–ø–∞–º—è—Ç—å",
            
            "üí° –í–ï–õ–ò–ö–û–ï –û–¢–ö–†–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüéØ –≠—Ç–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–∏–ª–æ —Ö–æ–¥ –∏—Å—Ç–æ—Ä–∏–∏.\n\n#–∏—Å—Ç–æ—Ä–∏—è #–æ—Ç–∫—Ä—ã—Ç–∏–µ #–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ"
        ]
        
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def get_science_image(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤"""
        science_images = [
            'https://images.unsplash.com/photo-1532094349884-543bc11b234d?w=500&fit=crop',
            'https://images.unsplash.com/photo-1563089145-599997674d42?w=500&fit=crop',
            'https://images.unsplash.com/photo-1554475900-0a0350e3fc7b?w=500&fit=crop'
        ]
        return random.choice(science_images)

    def get_tech_image(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å—Ç–æ–≤"""
        tech_images = [
            'https://images.unsplash.com/photo-1518709268805-4e9042af2176?w=500&fit=crop',
            'https://images.unsplash.com/photo-1517077304055-6e89abbf09b0?w=500&fit=crop',
            'https://images.unsplash.com/photo-1542831371-29b0f74f9713?w=500&fit=crop'
        ]
        return random.choice(tech_images)

    def get_historical_image(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å—Ç–æ–≤"""
        historical_images = [
            'https://images.unsplash.com/photo-1481627834876-b7833e8f5570?w=500&fit=crop',
            'https://images.unsplash.com/photo-1589652717521-10c0d092dea9?w=500&fit=crop',
            'https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=500&fit=crop'
        ]
        return random.choice(historical_images)

    def get_category_image(self, category):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        category_images = {
            'space': 'https://images.unsplash.com/photo-1446776653964-20c1d3a81b06?w=500&fit=crop',
            'technology': 'https://images.unsplash.com/photo-1518709268805-4e9042af2176?w=500&fit=crop',
            'photography': 'https://images.unsplash.com/photo-1516035069371-29a1b244cc32?w=500&fit=crop',
            'medicine': 'https://images.unsplash.com/photo-1559757148-5c350d0d3c56?w=500&fit=crop',
            'history': 'https://images.unsplash.com/photo-1481627834876-b7833e8f5570?w=500&fit=crop',
            'science': 'https://images.unsplash.com/photo-1532094349884-543bc11b234d?w=500&fit=crop'
        }
        return category_images.get(category, 'https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=500&fit=crop')

    def is_relevant_content(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –∏—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–µ—Ä–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π"""
        keywords = [
            '–ø–µ—Ä–≤—ã–π', '–ø–µ—Ä–≤–æ–µ', '–ø–µ—Ä–≤–∞—è', '–≤–ø–µ—Ä–≤—ã–µ', '–≤–ø–µ—Ä–≤—ã',
            '–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '—Ä–µ–≤–æ–ª—é—Ü–∏—è', '–ø—Ä–æ—Ä—ã–≤',
            '—Ä–µ–∫–æ—Ä–¥', '–∏—Å—Ç–æ—Ä–∏—è', '—Å–æ–∑–¥–∞–Ω', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω', '–∑–∞–ø—É—â–µ–Ω',
            '–æ–±–Ω–∞—Ä—É–∂–µ–Ω', '–Ω–∞—á–∞–ª–æ', '–≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ', '–ø–æ—è–≤–ª–µ–Ω–∏–µ',
            '–æ—Å–Ω–æ–≤–∞–Ω–∏–µ', '—Å–æ–∑–¥–∞–Ω–∏–µ', '–∏–∑–æ–±—Ä–µ—Ç', '–æ—Ç–∫—Ä—ã—Ç'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in keywords)

    def format_for_preview(self, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
        current_time = datetime.now()
        preview_text = f"üì∞ –ù–û–í–´–ô –ü–û–°–¢ –î–õ–Ø –ü–£–ë–õ–ò–ö–ê–¶–ò–ò\n\n{content['summary']}\n\n"
        
        if content.get('image_url'):
            preview_text += f"üñºÔ∏è –ü—Ä–∏–ª–æ–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n\n"
        
        preview_text += f"‚è∞ –ù–∞–π–¥–µ–Ω–æ: {current_time.strftime('%H:%M %d.%m.%Y')}"
        
        return preview_text

def setup_content_finder(db_manager=None):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–µ—Ä–µ–¥–∞—á–µ–π db_manager"""
    return ContentFinder(db_manager)
