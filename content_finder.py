# content_finder.py
import logging
import requests
from datetime import datetime
import random
import hashlib
from bs4 import BeautifulSoup
import re
import json

logger = logging.getLogger(__name__)

class ContentFinder:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–æ–≤
        self.post_hashes = set()
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.sources = [
            self.parse_science_news,
            self.parse_tech_news,
            self.parse_historical_facts,
            self.generate_ai_content
        ]

    def search_content(self, max_posts=3):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        found_content = []
        attempts = 0
        
        while len(found_content) < max_posts and attempts < 10:
            attempts += 1
            
            for source in self.sources:
                try:
                    content_list = source()
                    if content_list:
                        for content in content_list:
                            if self.is_unique_content(content) and len(found_content) < max_posts:
                                found_content.append(content)
                                self.post_hashes.add(self.get_content_hash(content))
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç: {content['title'][:50]}...")
                            
                            if len(found_content) >= max_posts:
                                break
                
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source.__name__}: {e}")
                    continue
                
                if len(found_content) >= max_posts:
                    break
        
        logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {len(found_content)}")
        return found_content

    def is_unique_content(self, content):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content_hash = self.get_content_hash(content)
        return content_hash not in self.post_hashes

    def get_content_hash(self, content):
        """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏"""
        text = content['title'] + content['summary']
        return hashlib.md5(text.encode()).hexdigest()

    def parse_science_news(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            # Naked Science
            url = "https://naked-science.ru"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = []
            news_items = soup.find_all('article', class_='news')[:5]
            
            for item in news_items:
                try:
                    title_elem = item.find('h2') or item.find('a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text().strip()
                    
                    # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                    description_elem = item.find('p') or item.find('div', class_='description')
                    description = description_elem.get_text().strip() if description_elem else ""
                    
                    if self.is_relevant_content(title + description):
                        full_text = self.get_article_content(item)
                        formatted_post = self.format_science_post(title, full_text or description)
                        
                        articles.append({
                            'title': title,
                            'summary': formatted_post,
                            'category': 'science',
                            'url': url,
                            'image_url': self.get_science_image(),
                            'found_date': datetime.now()
                        })
                        
                except Exception as e:
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    def parse_tech_news(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            # 3D News
            url = "https://3dnews.ru"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = []
            news_items = soup.find_all('article')[:5] or soup.find_all('div', class_='news-item')[:5]
            
            for item in news_items:
                try:
                    title_elem = item.find('h2') or item.find('h3') or item.find('a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text().strip()
                    
                    if self.is_relevant_content(title):
                        formatted_post = self.format_tech_post(title)
                        
                        articles.append({
                            'title': title,
                            'summary': formatted_post,
                            'category': 'technology',
                            'url': url,
                            'image_url': self.get_tech_image(),
                            'found_date': datetime.now()
                        })
                        
                except Exception:
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ—Ö–Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    def parse_historical_facts(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Wikipedia API –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'list': 'search',
                'srsearch': '–ø–µ—Ä–≤—ã–π –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –∏–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª—å –ø–µ—Ä–≤–æ–æ—Ç–∫—Ä—ã–≤–∞—Ç–µ–ª—å',
                'format': 'json',
                'srlimit': 5
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            articles = []
            for item in data.get('query', {}).get('search', [])[:3]:
                title = item.get('title', '')
                
                if self.is_relevant_content(title):
                    full_content = self.get_wikipedia_content(title)
                    if full_content:
                        formatted_post = self.format_historical_post(title, full_content)
                        
                        articles.append({
                            'title': title,
                            'summary': formatted_post,
                            'category': 'history',
                            'url': f"https://ru.wikipedia.org/wiki/{title.replace(' ', '_')}",
                            'image_url': self.get_historical_image(),
                            'found_date': datetime.now()
                        })
            
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤: {e}")
            return []

    def generate_ai_content(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        historical_events = [
            {
                'title': '–ü–µ—Ä–≤—ã–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø—É—Ç–Ω–∏–∫ –ó–µ–º–ª–∏',
                'content': """4 –æ–∫—Ç—è–±—Ä—è 1957 –≥–æ–¥–∞ —Å –∫–æ—Å–º–æ–¥—Ä–æ–º–∞ –ë–∞–π–∫–æ–Ω—É—Ä –±—ã–ª –∑–∞–ø—É—â–µ–Ω ¬´–°–ø—É—Ç–Ω–∏–∫-1¬ª ‚Äî –ø–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø—É—Ç–Ω–∏–∫ –ó–µ–º–ª–∏.

üìÖ –î–∞—Ç–∞: 4 –æ–∫—Ç—è–±—Ä—è 1957 –≥–æ–¥–∞
üè≥Ô∏è –°—Ç—Ä–∞–Ω–∞: –°–°–°–†
‚öñÔ∏è –ú–∞—Å—Å–∞: 83,6 –∫–≥
üïí –ü–µ—Ä–∏–æ–¥ –æ–±—Ä–∞—â–µ–Ω–∏—è: 96,2 –º–∏–Ω—É—Ç—ã

–ó–∞–ø—É—Å–∫ –ø–µ—Ä–≤–æ–≥–æ —Å–ø—É—Ç–Ω–∏–∫–∞ –¥–æ–∫–∞–∑–∞–ª –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ—Å–º–∏—á–µ—Å–∫–∏—Ö –∞–ø–ø–∞—Ä–∞—Ç–æ–≤ –∏ –æ—Ç–∫—Ä—ã–ª –¥–æ—Ä–æ–≥—É –¥–ª—è –ø–∏–ª–æ—Ç–∏—Ä—É–µ–º–æ–π –∫–æ—Å–º–æ–Ω–∞–≤—Ç–∏–∫–∏.""",
                'category': 'space'
            },
            {
                'title': '–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ —Ç–µ–ª–µ–≥—Ä–∞—Ñ–∞',
                'content': """–í 1837 –≥–æ–¥—É –°—ç–º—é—ç–ª –ú–æ—Ä–∑–µ —Å–æ–∑–¥–∞–ª –ø–µ—Ä–≤—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ–ª–µ–≥—Ä–∞—Ñ–Ω—ã–π –∞–ø–ø–∞—Ä–∞—Ç –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª –∞–∑–±—É–∫—É –ú–æ—Ä–∑–µ.

üìÖ –ì–æ–¥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è: 1837
üë®‚Äçüíº –ò–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª—å: –°—ç–º—é—ç–ª –ú–æ—Ä–∑–µ
üí° –ö–ª—é—á–µ–≤–æ–µ: –∞–∑–±—É–∫–∞ –ú–æ—Ä–∑–µ
üåç –ó–Ω–∞—á–µ–Ω–∏–µ: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è—Ö

–≠—Ç–æ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –∑–∞ —Å–µ–∫—É–Ω–¥—ã.""",
                'category': 'technology'
            },
            {
                'title': '–ü–µ—Ä–≤–∞—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è',
                'content': """–í 1826 –≥–æ–¥—É –ñ–æ–∑–µ—Ñ –ù–∏—Å–µ—Ñ–æ—Ä –ù—å–µ–ø—Å —Å–¥–µ–ª–∞–ª –ø–µ—Ä–≤—É—é –≤ –∏—Å—Ç–æ—Ä–∏–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é ¬´–í–∏–¥ –∏–∑ –æ–∫–Ω–∞ –≤ –õ–µ –ì—Ä–∞¬ª.

üìÖ –ì–æ–¥: 1826
üë®‚Äçüî¨ –ò–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª—å: –ñ–æ–∑–µ—Ñ –ù–∏—Å–µ—Ñ–æ—Ä –ù—å–µ–ø—Å
üñºÔ∏è –¢–µ—Ö–Ω–∏–∫–∞: –≥–µ–ª–∏–æ–≥—Ä–∞—Ñ–∏—è
‚è±Ô∏è –í—Ä–µ–º—è —ç–∫—Å–ø–æ–∑–∏—Ü–∏–∏: 8 —á–∞—Å–æ–≤

–≠—Ç–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –ø–æ–ª–æ–∂–∏–ª–∞ –Ω–∞—á–∞–ª–æ —Ä–∞–∑–≤–∏—Ç–∏—é —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∫–∞–∫ –∏—Å–∫—É—Å—Å—Ç–≤–∞ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.""",
                'category': 'photography'
            }
        ]
        
        articles = []
        for event in random.sample(historical_events, 2):
            formatted_post = self.format_ai_post(event['title'], event['content'])
            
            articles.append({
                'title': event['title'],
                'summary': formatted_post,
                'category': event['category'],
                'url': '',
                'image_url': self.get_category_image(event['category']),
                'found_date': datetime.now()
            })
        
        return articles

    def format_science_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—ã–π –ø–æ—Å—Ç –≤ —Å—Ç–∏–ª–µ –í–ö"""
        templates = [
            "üî¨ –ù–ê–£–ß–ù–û–ï –û–¢–ö–†–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüí´ –≠—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∏–µ –º–µ–Ω—è–µ—Ç –Ω–∞—à–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ –º–∏—Ä–µ –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.\n\n#–Ω–∞—É–∫–∞ #–æ—Ç–∫—Ä—ã—Ç–∏–µ #–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
            
            "üåå –ü–†–û–†–´–í –í –ù–ê–£–ö–ï\n\n{title}\n\n{content}\n\nüöÄ –£—á–µ–Ω—ã–µ —Å–æ–≤–µ—Ä—à–∏–ª–∏ –≤–∞–∂–Ω—ã–π —à–∞–≥ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏—Ä–æ–¥—ã.\n\n#–Ω–∞—É–∫–∞ #–ø—Ä–æ—Ä—ã–≤ #–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"
        ]
        
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def format_tech_post(self, title):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Å—Ç –≤ —Å—Ç–∏–ª–µ –í–ö"""
        templates = [
            "‚ö° –¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –†–ï–í–û–õ–Æ–¶–ò–Ø\n\n{title}\n\nüí° –≠—Ç–æ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ –º–µ–Ω—è–µ—Ç –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã—Ö –∑–∞–¥–∞—á –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.\n\n#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ #–±—É–¥—É—â–µ–µ",
            
            "ü§ñ –ò–ù–ù–û–í–ê–¶–ò–û–ù–ù–ê–Ø –†–ê–ó–†–ê–ë–û–¢–ö–ê\n\n{title}\n\nüöÄ –ü—Ä–æ–≥—Ä–µ—Å—Å –Ω–µ —Å—Ç–æ–∏—Ç –Ω–∞ –º–µ—Å—Ç–µ - —ç—Ç–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã technological —Ä–∞–∑–≤–∏—Ç–∏—è.\n\n#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ #–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"
        ]
        
        template = random.choice(templates)
        return template.format(title=title.upper())

    def format_historical_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –ø–æ—Å—Ç –≤ —Å—Ç–∏–ª–µ –í–ö"""
        templates = [
            "üèÜ –ò–°–¢–û–†–ò–ß–ï–°–ö–û–ï –°–û–ë–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüìö –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ —Å—Ç–∞–ª–æ –ø–æ–≤–æ—Ä–æ—Ç–Ω—ã–º –º–æ–º–µ–Ω—Ç–æ–º –≤ –∏—Å—Ç–æ—Ä–∏–∏ –∏ –æ–∫–∞–∑–∞–ª–æ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞.\n\n#–∏—Å—Ç–æ—Ä–∏—è #—Å–æ–±—ã—Ç–∏–µ #–ø–∞–º—è—Ç—å",
            
            "üí° –í–ï–õ–ò–ö–û–ï –û–¢–ö–†–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüéØ –≠—Ç–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–∏–ª–æ —Ö–æ–¥ –∏—Å—Ç–æ—Ä–∏–∏ –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –Ω–∞—à—É –∂–∏–∑–Ω—å —Å–µ–≥–æ–¥–Ω—è.\n\n#–∏—Å—Ç–æ—Ä–∏—è #–æ—Ç–∫—Ä—ã—Ç–∏–µ #–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ"
        ]
        
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def format_ai_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç AI-–ø–æ—Å—Ç –≤ —Å—Ç–∏–ª–µ –í–ö"""
        templates = [
            "üåü –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–ô –ü–†–û–†–´–í\n\n{title}\n\n{content}\n\nüí´ –≠—Ç–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –Ω–∞–≤—Å–µ–≥–¥–∞ –∏–∑–º–µ–Ω–∏–ª–æ –º–∏—Ä –∏ —Å—Ç–∞–ª–æ –≤–∞–∂–Ω–æ–π –≤–µ—Ö–æ–π –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞.\n\n#–∏—Å—Ç–æ—Ä–∏—è #–ø—Ä–æ—Ä—ã–≤ #–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ",
            
            "üéØ –ü–ï–†–í–´–ô –®–ê–ì\n\n{title}\n\n{content}\n\nüöÄ –° —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ –Ω–∞—á–∞–ª–∞—Å—å –Ω–æ–≤–∞—è —ç—Ä–∞, –∏–∑–º–µ–Ω–∏–≤—à–∞—è –ø—Ä–∏–≤—ã—á–Ω—ã–π —É–∫–ª–∞–¥ –∂–∏–∑–Ω–∏ –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ª—é–¥–µ–π.\n\n#–ø–µ—Ä–≤—ã–π #–∏—Å—Ç–æ—Ä–∏—è #–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ"
        ]
        
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def get_article_content(self, item):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏"""
        try:
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
            content_elems = item.find_all('p') + item.find_all('div', class_=re.compile('content|description|text'))
            
            content_parts = []
            for elem in content_elems[:2]:
                text = elem.get_text().strip()
                if len(text) > 50:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –±–ª–æ–∫–∏
                    content_parts.append(text)
            
            return ' '.join(content_parts)[:300] + '...' if content_parts else ""
            
        except Exception as e:
            return ""

    def get_wikipedia_content(self, title):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ Wikipedia"""
        try:
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'prop': 'extracts',
                'titles': title,
                'exintro': True,
                'explaintext': True,
                'format': 'json'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                extract = page_data.get('extract', '')
                if extract:
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü
                    first_para = extract.split('\n')[0]
                    return first_para[:400] + '...'
            
            return ""
            
        except Exception as e:
            return ""

    def get_science_image(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤"""
        science_images = [
            'https://images.unsplash.com/photo-1532094349884-543bc11b234d?w=500&fit=crop',
            'https://images.unsplash.com/photo-1563089145-599997674d42?w=500&fit=crop',
            'https://images.unsplash.com/photo-1554475900-0a0350e3fc7b?w=500&fit=crop'
        ]
        return random.choice(science_images)

    def get_tech_image(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å—Ç–æ–≤"""
        tech_images = [
            'https://images.unsplash.com/photo-1518709268805-4e9042af2176?w=500&fit=crop',
            'https://images.unsplash.com/photo-1517077304055-6e89abbf09b0?w=500&fit=crop',
            'https://images.unsplash.com/photo-1542831371-29b0f74f9713?w=500&fit=crop'
        ]
        return random.choice(tech_images)

    def get_historical_image(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å—Ç–æ–≤"""
        historical_images = [
            'https://images.unsplash.com/photo-1481627834876-b7833e8f5570?w=500&fit=crop',
            'https://images.unsplash.com/photo-1589652717521-10c0d092dea9?w=500&fit=crop',
            'https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=500&fit=crop'
        ]
        return random.choice(historical_images)

    def get_category_image(self, category):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        category_images = {
            'space': 'https://images.unsplash.com/photo-1446776653964-20c1d3a81b06?w=500&fit=crop',
            'technology': 'https://images.unsplash.com/photo-1518709268805-4e9042af2176?w=500&fit=crop',
            'photography': 'https://images.unsplash.com/photo-1516035069371-29a1b244cc32?w=500&fit=crop'
        }
        return category_images.get(category, 'https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=500&fit=crop')

    def is_relevant_content(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        keywords = [
            '–ø–µ—Ä–≤—ã–π', '–ø–µ—Ä–≤–æ–µ', '–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '—Ä–µ–≤–æ–ª—é—Ü–∏—è',
            '–ø—Ä–æ—Ä—ã–≤', '—Ä–µ–∫–æ—Ä–¥', '–∏—Å—Ç–æ—Ä–∏—è', '—Å–æ–∑–¥–∞–Ω', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω',
            '–∑–∞–ø—É—â–µ–Ω', '–æ–±–Ω–∞—Ä—É–∂–µ–Ω', '–Ω–∞—É—á–Ω—ã–π', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '–∏–Ω–Ω–æ–≤–∞—Ü',
            '–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π', '–≤–ø–µ—Ä–≤—ã–µ', '–Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '–ø—Ä–æ—Ä—ã–≤'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in keywords)

    def format_for_preview(self, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
        current_time = datetime.now()
        preview_text = f"üì∞ –ù–û–í–´–ô –ü–û–°–¢ –î–õ–Ø –ü–£–ë–õ–ò–ö–ê–¶–ò–ò\n\n{content['summary']}\n\n"
        
        if content.get('image_url'):
            preview_text += f"üñºÔ∏è –ü—Ä–∏–ª–æ–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n\n"
        
        preview_text += f"‚è∞ –ù–∞–π–¥–µ–Ω–æ: {current_time.strftime('%H:%M %d.%m.%Y')}"
        
        return preview_text

def setup_content_finder():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    return ContentFinder()
