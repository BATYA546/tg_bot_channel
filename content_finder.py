# content_finder.py
import logging
import requests
from datetime import datetime
import random
from bs4 import BeautifulSoup
import re

logger = logging.getLogger(__name__)

class ContentFinder:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def search_content(self, max_posts=2):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        found_content = []
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = [
            self.parse_historical_events,
            self.parse_science_news,
            self.generate_quality_content
        ]
        
        for source in sources:
            try:
                content = source()
                if content:
                    found_content.extend(content)
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(content)} –∑–∞–ø–∏—Å–µ–π")
                
                if len(found_content) >= max_posts:
                    break
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {e}")
                continue
        
        return found_content[:max_posts]

    def parse_historical_events(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Wikipedia API –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'list': 'search',
                'srsearch': '–ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –∏–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª—å',
                'format': 'json',
                'srlimit': 5
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            articles = []
            for item in data.get('query', {}).get('search', [])[:3]:
                title = item.get('title', '')
                snippet = item.get('snippet', '')
                
                # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –∏–∑ —Å–Ω–∏–ø–ø–µ—Ç–∞
                snippet = re.sub(r'<[^>]+>', '', snippet)
                
                if self.is_relevant_content(title + snippet):
                    full_content = self.get_wikipedia_content(title)
                    if full_content:
                        image_url = self.get_wikipedia_image(title)
                        post_text = self.format_wiki_post(title, full_content)
                        
                        articles.append({
                            'title': title,
                            'summary': post_text,
                            'category': 'history',
                            'url': f"https://ru.wikipedia.org/wiki/{title.replace(' ', '_')}",
                            'image_url': image_url,
                            'found_date': datetime.now()
                        })
            
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Wikipedia: {e}")
            return []

    def get_wikipedia_content(self, title):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏"""
        try:
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'prop': 'extracts',
                'titles': title,
                'explaintext': True,
                'format': 'json'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                extract = page_data.get('extract', '')
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
                if extract:
                    sentences = extract.split('.')
                    main_content = '.'.join(sentences[:3]) + '.'  # –ü–µ—Ä–≤—ã–µ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    return main_content.strip()
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return ""

    def get_wikipedia_image(self, title):
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        try:
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'prop': 'pageimages',
                'titles': title,
                'pithumbsize': 500,
                'format': 'json'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                thumbnail = page_data.get('thumbnail')
                if thumbnail:
                    return thumbnail.get('source', '')
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return ""

    def format_wiki_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–æ—Å—Ç –∏–∑ Wikipedia"""
        # –°–æ–∑–¥–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ—Å—Ç –≤ —Å—Ç–∏–ª–µ –≤–∞—à–µ–π –≥—Ä—É–ø–ø—ã –í–ö
        template = random.choice([
            "üèÜ –ò–°–¢–û–†–ò–ß–ï–°–ö–û–ï –°–û–ë–´–¢–ò–ï: {title}\n\n{content}\n\nüìö –≠—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–ª–æ–∂–∏–ª–æ –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–π —ç—Ä–µ –≤ —Å–≤–æ–µ–π –æ–±–ª–∞—Å—Ç–∏ –∏ –∏–∑–º–µ–Ω–∏–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞ –æ –≤–æ–∑–º–æ–∂–Ω–æ–º.\n\n#–∏—Å—Ç–æ—Ä–∏—è #–æ—Ç–∫—Ä—ã—Ç–∏–µ #–ø–µ—Ä–≤—ã–π",
            
            "üí° –ù–ê–£–ß–ù–´–ô –ü–†–û–†–´–í: {title}\n\n{content}\n\nüî¨ –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–µ—Ä–µ–≤–µ—Ä–Ω—É–ª–æ –ø—Ä–∏–≤—ã—á–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –æ—Ç–∫—Ä—ã–ª–æ –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.\n\n#–Ω–∞—É–∫–∞ #–ø—Ä–æ—Ä—ã–≤ #–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ",
            
            "üöÄ –¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –†–ï–í–û–õ–Æ–¶–ò–Ø: {title}\n\n{content}\n\n‚ö° –ò–Ω–Ω–æ–≤–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∞ –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏ –ª—é–¥–µ–π –∏ —Å—Ç–∞–ª–∞ –Ω–µ–æ—Ç—ä–µ–º–ª–µ–º–æ–π —á–∞—Å—Ç—å—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –º–∏—Ä–∞.\n\n#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #—Ä–µ–≤–æ–ª—é—Ü–∏—è #–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"
        ])
        
        return template.format(title=title.upper(), content=content)

    def parse_science_news(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            # –ü–∞—Ä—Å–∏–º –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
            url = "https://naked-science.ru"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = []
            news_items = soup.find_all('article', class_='news')[:3]
            
            for item in news_items:
                try:
                    title_elem = item.find('h2') or item.find('a')
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text().strip()
                    if self.is_relevant_content(title):
                        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                        link = title_elem.get('href', '')
                        if link and not link.startswith('http'):
                            link = url + link
                        
                        full_content = self.get_article_content(link) if link else title
                        post_text = self.format_news_post(title, full_content)
                        
                        articles.append({
                            'title': title,
                            'summary': post_text,
                            'category': 'science',
                            'url': link,
                            'image_url': '',
                            'found_date': datetime.now()
                        })
                        
                except Exception as e:
                    continue
                    
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    def get_article_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content_div = soup.find('div', class_='content') or soup.find('article')
            if content_div:
                paragraphs = content_div.find_all('p')[:2]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 –∞–±–∑–∞—Ü–∞
                content = ' '.join([p.get_text().strip() for p in paragraphs])
                return content[:300] + '...'  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏: {e}")
            return ""

    def format_news_post(self, title, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ø–æ—Å—Ç"""
        template = random.choice([
            "üî¨ –ù–ê–£–ß–ù–û–ï –û–¢–ö–†–´–¢–ò–ï\n\n{title}\n\n{content}\n\nüí´ –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–∫—Ä—É–∂–∞—é—â–µ–≥–æ –º–∏—Ä–∞.\n\n#–Ω–∞—É–∫–∞ #–æ—Ç–∫—Ä—ã—Ç–∏–µ #–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
            
            "üåç –ü–ï–†–í–´–ô –®–ê–ì: {title}\n\n{content}\n\nüéØ –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —Å—Ç–∞–ª–æ –æ—Ç–ø—Ä–∞–≤–Ω–æ–π —Ç–æ—á–∫–æ–π –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.\n\n#–ø–µ—Ä–≤—ã–π #–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ #–ø—Ä–æ–≥—Ä–µ—Å—Å"
        ])
        
        return template.format(title=title, content=content)

    def generate_quality_content(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∫–æ–≥–¥–∞ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        quality_posts = [
            {
                'title': '–ü–µ—Ä–≤—ã–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø—É—Ç–Ω–∏–∫ –ó–µ–º–ª–∏',
                'summary': "üõ∞Ô∏è –ü–ï–†–í–´–ô –ò–°–ö–£–°–°–¢–í–ï–ù–ù–´–ô –°–ü–£–¢–ù–ò–ö –ó–ï–ú–õ–ò\n\n4 –æ–∫—Ç—è–±—Ä—è 1957 –≥–æ–¥–∞ —Å –∫–æ—Å–º–æ–¥—Ä–æ–º–∞ –ë–∞–π–∫–æ–Ω—É—Ä –±—ã–ª –∑–∞–ø—É—â–µ–Ω ¬´–°–ø—É—Ç–Ω–∏–∫-1¬ª ‚Äî –ø–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø—É—Ç–Ω–∏–∫ –ó–µ–º–ª–∏. –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ –ø–æ–ª–æ–∂–∏–ª–æ –Ω–∞—á–∞–ª–æ –∫–æ—Å–º–∏—á–µ—Å–∫–æ–π —ç—Ä–µ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞.\n\n‚Ä¢ –î–∞—Ç–∞: 4 –æ–∫—Ç—è–±—Ä—è 1957 –≥–æ–¥–∞\n‚Ä¢ –°—Ç—Ä–∞–Ω–∞: –°–°–°–†\n‚Ä¢ –ú–∞—Å—Å–∞: 83,6 –∫–≥\n‚Ä¢ –ü–µ—Ä–∏–æ–¥ –æ–±—Ä–∞—â–µ–Ω–∏—è: 96,2 –º–∏–Ω—É—Ç—ã\n\nüåü –ó–∞–ø—É—Å–∫ –ø–µ—Ä–≤–æ–≥–æ —Å–ø—É—Ç–Ω–∏–∫–∞ –¥–æ–∫–∞–∑–∞–ª –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ—Å–º–∏—á–µ—Å–∫–∏—Ö –∞–ø–ø–∞—Ä–∞—Ç–æ–≤ –∏ –æ—Ç–∫—Ä—ã–ª –¥–æ—Ä–æ–≥—É –¥–ª—è –ø–∏–ª–æ—Ç–∏—Ä—É–µ–º–æ–π –∫–æ—Å–º–æ–Ω–∞–≤—Ç–∏–∫–∏. –≠—Ç–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Å—Ç–∞–ª–æ —Å–∏–º–≤–æ–ª–æ–º –Ω–∞—É—á–Ω–æ-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ –≤–¥–æ—Ö–Ω–æ–≤–∏–ª–æ —Ü–µ–ª–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ —É—á–µ–Ω—ã—Ö –∏ –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤.\n\n#–∫–æ—Å–º–æ—Å #—Å–ø—É—Ç–Ω–∏–∫ #–ø–µ—Ä–≤—ã–π #–°–°–°–† #–∏—Å—Ç–æ—Ä–∏—è",
                'category': 'space',
                'url': '',
                'image_url': 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Sputnik_1.jpg/500px-Sputnik_1.jpg',
                'found_date': datetime.now()
            },
            {
                'title': '–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞',
                'summary': "üìû –ò–ó–û–ë–†–ï–¢–ï–ù–ò–ï –¢–ï–õ–ï–§–û–ù–ê\n\n14 —Ñ–µ–≤—Ä–∞–ª—è 1876 –≥–æ–¥–∞ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ë–µ–ª–ª –ø–æ–¥–∞–ª –∑–∞—è–≤–∫—É –Ω–∞ –ø–∞—Ç–µ–Ω—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä–æ–µ –Ω–∞–∑–≤–∞–ª ¬´—Ç–µ–ª–µ—Ñ–æ–Ω¬ª. –≠—Ç–æ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –Ω–∞–≤—Å–µ–≥–¥–∞ –∏–∑–º–µ–Ω–∏–ª–æ —Å–ø–æ—Å–æ–±—ã –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É –ª—é–¥—å–º–∏.\n\n‚Ä¢ –ò–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª—å: –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ë–µ–ª–ª\n‚Ä¢ –î–∞—Ç–∞ –ø–∞—Ç–µ–Ω—Ç–∞: 7 –º–∞—Ä—Ç–∞ 1876 –≥–æ–¥–∞\n‚Ä¢ –ü–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞: ¬´–ú–∏—Å—Ç–µ—Ä –í–∞—Ç—Å–æ–Ω, –∏–¥–∏—Ç–µ —Å—é–¥–∞. –í—ã –º–Ω–µ –Ω—É–∂–Ω—ã¬ª\n\nüí° –¢–µ–ª–µ—Ñ–æ–Ω —Å—Ç–∞–ª –ø–µ—Ä–≤—ã–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ–º, –ø–æ–∑–≤–æ–ª–∏–≤—à–∏–º –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é —Ä–µ—á—å –Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ. –ó–∞ –ø–µ—Ä–≤—ã–µ 10 –ª–µ—Ç –±—ã–ª–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –±–æ–ª–µ–µ 100 000 –∞–ø–ø–∞—Ä–∞—Ç–æ–≤, —á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ—Å—Ç–∏ —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è.\n\n#—Ç–µ–ª–µ—Ñ–æ–Ω #–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ #–ë–µ–ª–ª #–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è #–∏—Å—Ç–æ—Ä–∏—è",
                'category': 'technology',
                'url': '',
                'image_url': 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Alexander_Graham_Bell.jpg/500px-Alexander_Graham_Bell.jpg',
                'found_date': datetime.now()
            },
            {
                'title': '–ü–µ—Ä–≤—ã–π –ø–æ–ª–µ—Ç –±—Ä–∞—Ç—å–µ–≤ –†–∞–π—Ç',
                'summary': "‚úàÔ∏è –ü–ï–†–í–´–ô –£–ü–†–ê–í–õ–Ø–ï–ú–´–ô –ü–û–õ–ï–¢\n\n17 –¥–µ–∫–∞–±—Ä—è 1903 –≥–æ–¥–∞ –±—Ä–∞—Ç—å—è –†–∞–π—Ç —Å–æ–≤–µ—Ä—à–∏–ª–∏ –ø–µ—Ä–≤—ã–π –≤ –º–∏—Ä–µ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π –ø–æ–ª–µ—Ç –Ω–∞ —Å–∞–º–æ–ª–µ—Ç–µ ¬´–§–ª–∞–π–µ—Ä-1¬ª. –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ –æ—Ç–∫—Ä—ã–ª–æ —ç—Ä—É –∞–≤–∏–∞—Ü–∏–∏.\n\n‚Ä¢ –î–∞—Ç–∞: 17 –¥–µ–∫–∞–±—Ä—è 1903 –≥–æ–¥–∞\n‚Ä¢ –ú–µ—Å—Ç–æ: –ö–∏—Ç—Ç–∏ –•–æ–∫, –°–µ–≤–µ—Ä–Ω–∞—è –ö–∞—Ä–æ–ª–∏–Ω–∞\n‚Ä¢ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª–µ—Ç–∞: 12 —Å–µ–∫—É–Ω–¥\n‚Ä¢ –î–∏—Å—Ç–∞–Ω—Ü–∏—è: 36,5 –º–µ—Ç—Ä–æ–≤\n\nüöÄ –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Å–∫—Ä–æ–º–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø–µ—Ä–≤–æ–≥–æ –ø–æ–ª–µ—Ç–∞, –æ–Ω –¥–æ–∫–∞–∑–∞–ª –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω—É—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ –ø–æ–ª–µ—Ç–∞ —Ç—è–∂–µ–ª–µ–µ –≤–æ–∑–¥—É—Ö–∞. –í—Å–µ–≥–æ –≤ —Ç–æ—Ç –¥–µ–Ω—å –±—ã–ª–æ —Å–æ–≤–µ—Ä—à–µ–Ω–æ 4 –ø–æ–ª–µ—Ç–∞, —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –ø—Ä–æ–¥–æ–ª–∂–∞–ª—Å—è 59 —Å–µ–∫—É–Ω–¥.\n\n#–∞–≤–∏–∞—Ü–∏—è #–†–∞–π—Ç #–ø–µ—Ä–≤—ã–π_–ø–æ–ª–µ—Ç #–∏—Å—Ç–æ—Ä–∏—è #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
                'category': 'aviation',
                'url': '',
                'image_url': 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/First_flight2.jpg/500px-First_flight2.jpg',
                'found_date': datetime.now()
            }
        ]
        
        return random.sample(quality_posts, 2)

    def is_relevant_content(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        keywords = [
            '–ø–µ—Ä–≤—ã–π', '–ø–µ—Ä–≤–æ–µ', '–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '—Ä–µ–≤–æ–ª—é—Ü–∏—è',
            '–ø—Ä–æ—Ä—ã–≤', '—Ä–µ–∫–æ—Ä–¥', '–∏—Å—Ç–æ—Ä–∏—è', '—Å–æ–∑–¥–∞–Ω', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω',
            '–∑–∞–ø—É—â–µ–Ω', '–æ–±–Ω–∞—Ä—É–∂–µ–Ω', '–Ω–∞—É—á–Ω—ã–π', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in keywords)

    def format_for_preview(self, content):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
        return f"üì∞ –ü–†–ï–î–ü–†–û–°–ú–û–¢–† –ü–û–°–¢–ê\n\n{content['summary']}\n\n‚è∞ –ù–∞–π–¥–µ–Ω–æ: {content['found_date'].strftime('%H:%M %d.%m.%Y')}"

def setup_content_finder():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    return ContentFinder()
