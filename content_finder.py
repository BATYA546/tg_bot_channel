# content_finder.py
import logging
import requests
from datetime import datetime
import random
import hashlib
from bs4 import BeautifulSoup
import re
import time
import urllib.parse
import feedparser

logger = logging.getLogger(__name__)

class ContentFinder:
    def __init__(self, db_manager=None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        self.db_manager = db_manager
        self.post_hashes = set()
        if db_manager:
            self.load_existing_hashes()
        
        self.sources = [
            self.parse_science_news,
            self.parse_tech_news,
            self.parse_historical_facts
        ]

    def load_existing_hashes(self):
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ñ…ÐµÑˆÐ¸ Ð¸Ð· Ð‘Ð”"""
        try:
            conn = self.db_manager.get_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT title, content FROM found_content')
            existing_posts = cursor.fetchall()
            
            for title, content in existing_posts:
                text = title + content[:200]
                content_hash = hashlib.md5(text.encode()).hexdigest()
                self.post_hashes.add(content_hash)
                
            logger.info(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(self.post_hashes)} Ñ…ÐµÑˆÐµÐ¹ Ð¸Ð· Ð‘Ð”")
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ñ…ÐµÑˆÐµÐ¹: {e}")

    def search_content(self, max_posts=3):
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¿Ð¾Ð¸ÑÐºÐ° ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°"""
        logger.info("ðŸ” ÐÐ°Ñ‡Ð¸Ð½Ð°ÑŽ Ð¿Ð¾Ð¸ÑÐº ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°...")
        
        found_content = []
        
        for source in self.sources:
            try:
                if len(found_content) >= max_posts:
                    break
                    
                content_list = source()
                if content_list:
                    for content in content_list:
                        if self.is_unique_content(content) and len(found_content) < max_posts:
                            found_content.append(content)
                            content_hash = self.get_content_hash(content)
                            self.post_hashes.add(content_hash)
                            logger.info(f"âœ… ÐÐ°Ð¹Ð´ÐµÐ½ Ð¿Ð¾ÑÑ‚: {content['title'][:50]}...")
            except Exception as e:
                logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°: {e}")
                continue
        
        logger.info(f"ðŸŽ¯ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¾Ð²: {len(found_content)}")
        return found_content

    def is_unique_content(self, content):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°"""
        content_hash = self.get_content_hash(content)
        
        if content_hash in self.post_hashes:
            return False
        
        if self.db_manager and self.is_content_in_db(content):
            return False
            
        return True

    def is_content_in_db(self, content):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Ð² Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        try:
            conn = self.db_manager.get_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT id FROM found_content WHERE title LIKE %s', (f"%{content['title'][:30]}%",))
            result = cursor.fetchone()
            return result is not None
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð‘Ð”: {e}")
            return False

    def get_content_hash(self, content):
        """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ñ…ÐµÑˆ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°"""
        text = content['title'] + content['summary'][:100]
        return hashlib.md5(text.encode()).hexdigest()

    def parse_science_news(self):
        """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ñ… Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹"""
        try:
            articles = []
            url = "https://naked-science.ru/rss.xml"
            
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                
                for entry in feed.entries[:5]:
                    title = entry.title
                    summary = entry.get('summary', '') or entry.get('description', '')
                    
                    if self.is_relevant_content(title + summary):
                        image_url = self.get_science_image()
                        formatted_post = self.format_science_post(title, summary)
                        
                        articles.append({
                            'title': title,
                            'summary': formatted_post,
                            'category': 'science',
                            'url': entry.link,
                            'image_url': image_url,
                            'found_date': datetime.now()
                        })
            
            return articles
            
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ñ… Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹: {e}")
            return []

    def parse_tech_news(self):
        """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹"""
        try:
            articles = []
            url = "https://3dnews.ru/news/rss/"
            
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                
                for entry in feed.entries[:5]:
                    title = entry.title
                    summary = entry.get('summary', '') or entry.get('description', '')
                    
                    if self.is_relevant_content(title + summary):
                        image_url = self.get_tech_image()
                        formatted_post = self.format_tech_post(title, summary)
                        
                        articles.append({
                            'title': title,
                            'summary': formatted_post,
                            'category': 'technology',
                            'url': entry.link,
                            'image_url': image_url,
                            'found_date': datetime.now()
                        })
            
            return articles
            
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ñ‚ÐµÑ…Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹: {e}")
            return []

    def parse_historical_facts(self):
        """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ„Ð°ÐºÑ‚Ð¾Ð²"""
        try:
            articles = []
            
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Wikipedia API
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'list': 'search',
                'srsearch': 'Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð¸Ð·Ð¾Ð±Ñ€ÐµÑ‚ÐµÐ½Ð¸Ðµ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ',
                'format': 'json',
                'srlimit': 5
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            for item in data.get('query', {}).get('search', [])[:3]:
                title = item.get('title', '')
                
                if self.is_relevant_content(title):
                    full_content = self.get_wikipedia_content(title)
                    if full_content:
                        image_url = self.get_historical_image()
                        formatted_post = self.format_historical_post(title, full_content)
                        
                        articles.append({
                            'title': title,
                            'summary': formatted_post,
                            'category': 'history',
                            'url': f"https://ru.wikipedia.org/wiki/{title.replace(' ', '_')}",
                            'image_url': image_url,
                            'found_date': datetime.now()
                        })
            
            return articles
            
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ„Ð°ÐºÑ‚Ð¾Ð²: {e}")
            return []

    def get_wikipedia_content(self, title):
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð¸Ð· Wikipedia"""
        try:
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'prop': 'extracts',
                'titles': title,
                'exintro': True,
                'explaintext': True,
                'format': 'json'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                extract = page_data.get('extract', '')
                if extract:
                    first_para = extract.split('\n')[0]
                    return first_para[:400] + '...'
            
            return ""
            
        except Exception as e:
            return ""

    def format_science_post(self, title, content):
        """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ð¹ Ð¿Ð¾ÑÑ‚"""
        templates = [
            "ðŸ”¬ ÐÐÐ£Ð§ÐÐžÐ• ÐžÐ¢ÐšÐ Ð«Ð¢Ð˜Ð•\n\n{title}\n\n{content}\n\nðŸ’« Ð­Ñ‚Ð¾ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ð¼ÐµÐ½ÑÐµÑ‚ Ð½Ð°ÑˆÐ¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¾ Ð¼Ð¸Ñ€Ðµ\n\n#Ð½Ð°ÑƒÐºÐ° #Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ #Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ",
            "ðŸŒŒ ÐŸÐ ÐžÐ Ð«Ð’ Ð’ ÐÐÐ£ÐšÐ•\n\n{title}\n\n{content}\n\nðŸš€ Ð£Ñ‡ÐµÐ½Ñ‹Ðµ ÑÐ¾Ð²ÐµÑ€ÑˆÐ¸Ð»Ð¸ Ð²Ð°Ð¶Ð½Ñ‹Ð¹ ÑˆÐ°Ð³\n\n#Ð½Ð°ÑƒÐºÐ° #Ð¿Ñ€Ð¾Ñ€Ñ‹Ð² #Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ"
        ]
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def format_tech_post(self, title, content):
        """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾ÑÑ‚"""
        templates = [
            "âš¡ Ð¢Ð•Ð¥ÐÐžÐ›ÐžÐ“Ð˜Ð§Ð•Ð¡ÐšÐÐ¯ Ð Ð•Ð’ÐžÐ›Ð®Ð¦Ð˜Ð¯\n\n{title}\n\n{content}\n\nðŸ’¡ Ð­Ñ‚Ð¾ Ð¸Ð·Ð¾Ð±Ñ€ÐµÑ‚ÐµÐ½Ð¸Ðµ Ð¼ÐµÐ½ÑÐµÑ‚ Ð¿Ð¾Ð´Ñ…Ð¾Ð´\n\n#Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¸ #Ð¸Ð½Ð½Ð¾Ð²Ð°Ñ†Ð¸Ð¸ #Ð±ÑƒÐ´ÑƒÑ‰ÐµÐµ",
            "ðŸ¤– Ð˜ÐÐÐžÐ’ÐÐ¦Ð˜ÐžÐÐÐÐ¯ Ð ÐÐ—Ð ÐÐ‘ÐžÐ¢ÐšÐ\n\n{title}\n\n{content}\n\nðŸš€ ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ Ð½Ðµ ÑÑ‚Ð¾Ð¸Ñ‚ Ð½Ð° Ð¼ÐµÑÑ‚Ðµ\n\n#Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¸ #Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° #Ð¸Ð½Ð½Ð¾Ð²Ð°Ñ†Ð¸Ð¸"
        ]
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def format_historical_post(self, title, content):
        """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾ÑÑ‚"""
        templates = [
            "ðŸ† Ð˜Ð¡Ð¢ÐžÐ Ð˜Ð§Ð•Ð¡ÐšÐžÐ• Ð¡ÐžÐ‘Ð«Ð¢Ð˜Ð•\n\n{title}\n\n{content}\n\nðŸ“š Ð­Ñ‚Ð¾ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ ÑÑ‚Ð°Ð»Ð¾ Ð¿Ð¾Ð²Ð¾Ñ€Ð¾Ñ‚Ð½Ñ‹Ð¼ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚Ð¾Ð¼\n\n#Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ #ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ #Ð¿Ð°Ð¼ÑÑ‚ÑŒ",
            "ðŸ’¡ Ð’Ð•Ð›Ð˜ÐšÐžÐ• ÐžÐ¢ÐšÐ Ð«Ð¢Ð˜Ð•\n\n{title}\n\n{content}\n\nðŸŽ¯ Ð­Ñ‚Ð¾ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ð»Ð¾ Ñ…Ð¾Ð´ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸\n\n#Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ #Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ #Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ðµ"
        ]
        template = random.choice(templates)
        return template.format(title=title.upper(), content=content)

    def get_science_image(self):
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ñ… Ð¿Ð¾ÑÑ‚Ð¾Ð²"""
        science_images = [
            'https://images.unsplash.com/photo-1532094349884-543bc11b234d?w=500&fit=crop',
            'https://images.unsplash.com/photo-1563089145-599997674d42?w=500&fit=crop',
        ]
        return random.choice(science_images)

    def get_tech_image(self):
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ð¾ÑÑ‚Ð¾Ð²"""
        tech_images = [
            'https://images.unsplash.com/photo-1518709268805-4e9042af2176?w=500&fit=crop',
            'https://images.unsplash.com/photo-1517077304055-6e89abbf09b0?w=500&fit=crop',
        ]
        return random.choice(tech_images)

    def get_historical_image(self):
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ð¾ÑÑ‚Ð¾Ð²"""
        historical_images = [
            'https://images.unsplash.com/photo-1481627834876-b7833e8f5570?w=500&fit=crop',
            'https://images.unsplash.com/photo-1589652717521-10c0d092dea9?w=500&fit=crop',
        ]
        return random.choice(historical_images)

    def is_relevant_content(self, text):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°"""
        keywords = [
            'Ð¿ÐµÑ€Ð²Ñ‹Ð¹', 'Ð¿ÐµÑ€Ð²Ð¾Ðµ', 'Ð¸Ð·Ð¾Ð±Ñ€ÐµÑ‚ÐµÐ½Ð¸Ðµ', 'Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ', 'Ñ€ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ',
            'Ð¿Ñ€Ð¾Ñ€Ñ‹Ð²', 'Ñ€ÐµÐºÐ¾Ñ€Ð´', 'Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ', 'ÑÐ¾Ð·Ð´Ð°Ð½', 'Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½',
            'Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½', 'Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½', 'Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ð¹', 'Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ñ'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in keywords)

    def format_for_preview(self, content):
        """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð°"""
        current_time = datetime.now()
        preview_text = f"ðŸ“° ÐÐžÐ’Ð«Ð™ ÐŸÐžÐ¡Ð¢ Ð”Ð›Ð¯ ÐŸÐ£Ð‘Ð›Ð˜ÐšÐÐ¦Ð˜Ð˜\n\n{content['summary']}\n\n"
        
        if content.get('image_url'):
            preview_text += f"ðŸ–¼ï¸ ÐŸÑ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ\n\n"
        
        preview_text += f"â° ÐÐ°Ð¹Ð´ÐµÐ½Ð¾: {current_time.strftime('%H:%M %d.%m.%Y')}"
        
        return preview_text

def setup_content_finder(db_manager=None):
    """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð¿Ð¾Ð¸ÑÐºÐ° ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°"""
    return ContentFinder(db_manager)
